{"cells":[{"cell_type":"markdown","metadata":{"id":"aMASY5gD9L0K"},"source":["# **Lab1: Regression**\n","In *lab 1*, you need to finish:\n","\n","1.  Basic Part: Implement the regression model to predict people's grip force from their weight.\n","You can use either Matrix Inversion or Gradient Descent.\n","\n","\n","> *   Step 1: Split Data\n","> *   Step 2: Preprocess Data\n","> *   Step 3: Implement Regression\n","> *   Step 4: Make Prediction\n","> *   Step 5: Train Model and Generate Result\n","\n","2.  Advanced Part: Implementing a regression model to predict grip force in a different way (for example, with more variables) than the basic part\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yNpd_FfX_BXI"},"source":["---\n","# 1. Basic Part (50%)\n","In the first part, you need to implement the regression to predict grip force\n","\n","Please save the prediction result in a CSV file and submit it to Kaggle"]},{"cell_type":"markdown","metadata":{"id":"egBMMLGV_X_x"},"source":["### Import Packages\n","\n","> Note: You **cannot** import any other package\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WhhUTua487C-"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import csv\n","import math\n","import random"]},{"cell_type":"markdown","metadata":{"id":"8iuXHvhLALwz"},"source":["### Global attributes\n","Define the global attributes\\\n","You can also add your own global attributes here"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"wXZVhdp8-flF"},"outputs":[],"source":["training_dataroot = 'lab1_basic_training.csv' # Training data file file named as 'lab1_basic_training.csv'\n","testing_dataroot = 'lab1_basic_testing.csv'   # Testing data file named as 'lab1_basic_testing.csv'\n","output_dataroot = 'lab1_basic.csv' # Output file will be named as 'lab1_basic.csv'\n","\n","training_datalist =  [] # Training datalist, saved as numpy array\n","testing_datalist =  [] # Testing datalist, saved as numpy array\n","\n","output_datalist =  [] # Your prediction, should be a list with 100 elements"]},{"cell_type":"markdown","metadata":{"id":"IyTqIRxQAtWj"},"source":["### Load the Input File\n","First, load the basic input file **lab1_basic_training.csv** and **lab1_basic_testing.csv**\n","\n","Input data would be stored in *training_datalist* and *testing_datalist*"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"KUzYjoq9AwRp"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 55.4      32.8    ]\n"," [ 53.6      39.4    ]\n"," [ 78.4      52.6    ]\n"," ...\n"," [ 65.3      46.     ]\n"," [ 62.7     125.68257]\n"," [ 59.7      38.9    ]]\n","[[ 53.6 ]\n"," [ 51.54]\n"," [ 79.7 ]\n"," [ 56.68]\n"," [ 68.5 ]\n"," [ 64.8 ]\n"," [ 57.4 ]\n"," [ 63.5 ]\n"," [ 63.3 ]\n"," [ 61.4 ]\n"," [ 69.9 ]\n"," [ 92.5 ]\n"," [ 83.1 ]\n"," [ 51.3 ]\n"," [ 64.5 ]\n"," [ 64.2 ]\n"," [ 64.8 ]\n"," [ 57.3 ]\n"," [ 55.5 ]\n"," [ 67.4 ]\n"," [ 84.  ]\n"," [ 66.4 ]\n"," [ 84.8 ]\n"," [ 82.3 ]\n"," [ 51.4 ]\n"," [ 91.4 ]\n"," [ 77.7 ]\n"," [ 56.4 ]\n"," [ 66.3 ]\n"," [ 52.8 ]\n"," [ 82.9 ]\n"," [ 68.9 ]\n"," [ 64.2 ]\n"," [ 64.1 ]\n"," [ 76.1 ]\n"," [ 57.72]\n"," [ 45.22]\n"," [ 90.6 ]\n"," [ 44.1 ]\n"," [ 60.9 ]\n"," [ 50.3 ]\n"," [ 69.  ]\n"," [ 79.8 ]\n"," [ 59.6 ]\n"," [ 51.5 ]\n"," [ 78.7 ]\n"," [ 56.4 ]\n"," [ 53.6 ]\n"," [ 52.1 ]\n"," [ 66.4 ]\n"," [ 75.2 ]\n"," [ 56.  ]\n"," [ 60.  ]\n"," [ 69.1 ]\n"," [ 62.  ]\n"," [ 73.5 ]\n"," [ 59.8 ]\n"," [ 79.1 ]\n"," [ 52.9 ]\n"," [ 85.  ]\n"," [ 82.6 ]\n"," [ 48.8 ]\n"," [ 84.6 ]\n"," [ 80.4 ]\n"," [ 73.5 ]\n"," [ 86.7 ]\n"," [ 57.8 ]\n"," [ 49.2 ]\n"," [ 84.4 ]\n"," [ 65.5 ]\n"," [ 89.9 ]\n"," [ 66.96]\n"," [ 85.1 ]\n"," [ 79.7 ]\n"," [104.9 ]\n"," [ 54.28]\n"," [ 56.5 ]\n"," [ 68.5 ]\n"," [ 76.92]\n"," [ 97.  ]\n"," [ 67.1 ]\n"," [ 72.5 ]\n"," [ 63.3 ]\n"," [ 63.9 ]\n"," [ 72.8 ]\n"," [ 65.3 ]\n"," [ 51.6 ]\n"," [ 62.4 ]\n"," [ 52.4 ]\n"," [ 58.4 ]\n"," [ 48.2 ]\n"," [ 75.  ]\n"," [ 55.9 ]\n"," [ 55.3 ]\n"," [ 78.3 ]\n"," [ 55.7 ]\n"," [ 86.9 ]\n"," [ 70.5 ]\n"," [ 83.5 ]\n"," [ 52.5 ]]\n"]}],"source":["# Read input csv to datalist\n","with open(training_dataroot, newline=\"\") as csvfile:\n","    training_datalist = pd.read_csv(training_dataroot).to_numpy()\n","\n","with open(testing_dataroot, newline=\"\") as csvfile:\n","    testing_datalist = pd.read_csv(testing_dataroot).to_numpy()\n","\n","# --- DEBUG ---\n","print(training_datalist)\n","print(testing_datalist)"]},{"cell_type":"markdown","metadata":{"id":"QFXG-axpAcom"},"source":["### Implement the Regression Model\n","\n","> Note: It is recommended to use the functions we defined, you can also define your own functions"]},{"cell_type":"markdown","metadata":{"id":"9bqYH_MvBv4v"},"source":["#### Step 1: Split Data\n","Split data in *training_datalist* into training dataset and validation dataset\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6K2QUnt-A-1r"},"outputs":[],"source":["def SplitData(data, split_ratio):\n","    \"\"\"\n","    Splits the given dataset into training and validation sets based on the specified split ratio.\n","\n","    Parameters:\n","    - data (numpy.ndarray): The dataset to be split. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n","    - split_ratio (float): The ratio of the data to be used for training. For example, a value of 0.8 means 80% of the data will be used for training and the remaining 20% for validation.\n","\n","    Returns:\n","    - training_data (numpy.ndarray): The portion of the dataset used for training.\n","    - validation_data (numpy.ndarray): The portion of the dataset used for validation.\n","\n","    \"\"\"\n","    training_data = []\n","    validation_data = []\n","\n","    # TODO - Done\n","    data_size = len(data)\n","    training_data = data[: math.floor(data_size * split_ratio)]\n","    validation_data = data[math.floor(data_size * split_ratio) :]\n","\n","    return training_data, validation_data"]},{"cell_type":"markdown","metadata":{"id":"-miSLyewCeME"},"source":["#### Step 2: Preprocess Data\n","Handle unreasonable data and missing data\n","\n","> Hint 1: Outliers and missing data can be addressed by either removing them or replacing them using statistical methods (e.g., the mean of all data).\n","\n","> Hint 2: Missing data are represented as `np.nan`, so functions like `np.isnan()` can be used to detect them.\n","\n","> Hint 3: Methods such as the Interquartile Range (IQR) can help detect outliers"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"jR4TYnwwCrci"},"outputs":[],"source":["def PreprocessDataBasic(data):\n","    \"\"\"\n","    Preprocess the given dataset and return the result.\n","\n","    Parameters:\n","    - data (numpy.ndarray): The dataset to preprocess. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n","\n","    Returns:\n","    - preprocessedData (numpy.ndarray): Preprocessed data.\n","    \"\"\"\n","    preprocessedData = data\n","\n","    # TODO\n","    print(f\"{preprocessedData=}\")\n","\n","    data_removed_nan = np.array(\n","        [i for i in preprocessedData if not np.isnan(i[0]) and not np.isnan(i[1])]\n","    )\n","\n","    (x_mean, y_mean) = np.mean(data_removed_nan, axis=0).tolist()\n","    print(f\"{(x_mean, y_mean)=}\")\n","\n","    x_q25, x_q50, x_q75 = np.percentile(data_removed_nan[:, 0], [25, 50, 75]).tolist()\n","    y_q25, y_q50, y_q75 = np.percentile(data_removed_nan[:, 1], [25, 50, 75]).tolist()\n","    print(f\"{(x_q25, x_q75)=}, {(y_q25, y_q75)=}\")\n","\n","    x_iqr = x_q75 - x_q25\n","    y_iqr = y_q75 - y_q25\n","    print(f\"{x_iqr=}, {y_iqr=}\")\n","\n","    x_ac_range = (x_q25 - 3 * x_iqr, x_q75 + 3 * x_iqr)\n","    y_ac_range = (y_q25 - 3 * y_iqr, y_q75 + 3 * y_iqr)\n","    print(f\"{x_ac_range=}, {y_ac_range=}\")\n","\n","    for i in preprocessedData:\n","        # nan value\n","        if np.isnan(i[0]):\n","            i[0] = x_mean\n","        if np.isnan(i[1]):\n","            i[1] = y_mean\n","\n","        # extreme outlier\n","        if not x_ac_range[0] <= i[0] <= x_ac_range[1]:\n","            i[0] = x_q50\n","        if not y_ac_range[0] <= i[1] <= y_ac_range[1]:\n","            i[1] = y_q50\n","\n","    return preprocessedData\n","\n","    # res = []\n","    # for i in preprocessedData:\n","    #     # nan value\n","    #     if (\n","    #         np.isnan(i[0])\n","    #         or np.isnan(i[1])\n","    #         or not x_ac_range[0] <= i[0] <= x_ac_range[1]\n","    #         or not y_ac_range[0] <= i[1] <= y_ac_range[1]\n","    #     ):\n","    #         continue\n","\n","    #     res.append(i)\n","\n","    # return np.array(res)"]},{"cell_type":"markdown","metadata":{"id":"csS9lL8DCzZO"},"source":["### Step 3: Implement Regression\n","You have to use Gradient Descent to finish this part"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"n8ftprTRC0Na"},"outputs":[],"source":["def RegressionBasic(dataset):\n","    \"\"\"\n","    Performs regression on the given dataset and return the coefficients.\n","\n","    Parameters:\n","    - dataset (numpy.ndarray): A 2D array where each row represents a data point.\n","\n","    Returns:\n","    - w (numpy.ndarray): The coefficients of the regression model. For example, y = w[0] + w[1] * x + w[2] * x^2 + ...\n","    \"\"\"\n","\n","    X = dataset[:, :1]\n","    y = dataset[:, 1]\n","    # print(f\"{np.sum(y)=}\")\n","\n","    # print(f\"{X=}, {y=}\")\n","\n","    # <TODO>: Decide on the degree of the polynomial\n","    degree = 2  # For example, quadratic regression\n","\n","    # Add polynomial features to X\n","    X_poly = np.ones((X.shape[0], 1))  # Add intercept term (column of ones)\n","    for d in range(1, degree + 1):\n","        X_poly = np.hstack((X_poly, X**d))  # Add x^d terms to feature matrix\n","    # print(f\"{X_poly=}\")\n","\n","    # Initialize coefficients (weights) to zero\n","    # Number of features (including intercept and polynomial terms)\n","    num_dimensions = X_poly.shape[1]\n","    w = np.zeros(num_dimensions)  # shihtl> 這個 w 就是我們要解的東西\n","    # print(f\"{num_dimensions=}, {w=}\")\n","\n","    # <TODO>: Set hyperparameters\n","    num_iteration = 70_0000\n","    learning_rate = 0.00000001\n","\n","    # Gradient Descent\n","    m = len(y)  # Number of data points\n","    for iteration in range(2, 2 + num_iteration):\n","        # learning_rate = 0.001 * (1 / math.log(math.log(iteration, 1.00000001), 1.00000001))\n","        # print(f\"{learning_rate=}\")\n","        # <TODO>: Prediction using current weights and compute error\n","        # print(f\"{X_poly.shape=}, {w.shape=}\")\n","        y_hat = X_poly @ w\n","        d = y - y_hat\n","        # print(f\"{d=}, {y_hat=}, {len(y)=}\")\n","\n","        # <TODO>: Compute gradient\n","        g = -2 * ((np.transpose(d) @ X_poly) / m)\n","        # g = -2 * (np.matmul(np.transpose(d), X_poly))\n","\n","        # <TODO>: Update the weights\n","        w = w - learning_rate * g\n","\n","        # <TODO>: Optionally, print the cost every 100 iterations\n","        if iteration % 10000 == 0:\n","            cost = np.sum((y - y_hat) ** 2) / m\n","            print(f\"Iteration {iteration}, Cost: {cost}\")\n","        # cost = np.sum((y - y_hat) ** 2) / len(y)\n","        # print(f\"Iteration {iteration}, Cost: {cost}\\n\\n\")\n","\n","    return w"]},{"cell_type":"markdown","metadata":{"id":"inqQ4lh8DFY6"},"source":["### Step 4: Make Prediction\n","Make prediction of testing dataset and store the value in *output_datalist*"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"WwGE2qjgDLwt"},"outputs":[],"source":["def MakePredictionBasic(w, test_dataset):\n","    \"\"\"\n","    Predicts the output for a given test dataset using a regression model.\n","\n","    Parameters:\n","    - w (numpy.ndarray): The coefficients of the model, where each element corresponds to\n","                               a coefficient for the respective power of the independent variable.\n","    - test_dataset (numpy.ndarray): A 1D array containing the input values (independent variable)\n","                                          for which predictions are to be made.\n","\n","    Returns:\n","    - list/numpy.ndarray: A list or 1d array of predicted values corresponding to each input value in the test dataset.\n","    \"\"\"\n","    prediction = []\n","\n","    # TODO\n","    for data in test_dataset:\n","        # prediction.append(w[0] + w[1] * data)\n","        prediction.append(w[0] + w[1] * data + w[2] * data * data)\n","\n","    return np.array(prediction)"]},{"cell_type":"markdown","metadata":{"id":"-q4qKXbDDmG9"},"source":["### Step 5: Train Model and Generate Result\n","\n","Use the above functions to train your model on training dataset, and predict the answer of testing dataset.\n","\n","Save your predicted values in `output_datalist`\n","\n","> Notice: **Remember to inclue the coefficients of your model in the report**\n","\n"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"coo82WvZDpMq"},"outputs":[{"name":"stdout","output_type":"stream","text":["preprocessedData=array([[55.4, 32.8],\n","       [53.6, 39.4],\n","       [78.4, 52.6],\n","       ...,\n","       [70.9, 49.1],\n","       [63.2, 53.2],\n","       [54.4, 40.4]])\n","(x_mean, y_mean)=(66.0059911301429, 45.015004475555365)\n","(x_q25, x_q75)=(57.7, 74.6), (y_q25, y_q75)=(33.4, 53.9)\n","x_iqr=16.89999999999999, y_iqr=20.5\n","x_ac_range=(7.000000000000028, 125.29999999999997), y_ac_range=(-28.1, 115.4)\n","preprocessedData=array([[50. , 37.7],\n","       [64.9, 49.9],\n","       [67.1, 39.1],\n","       ...,\n","       [65.3, 46. ],\n","       [62.7, 46. ],\n","       [59.7, 38.9]])\n","(x_mean, y_mean)=(66.03616234505861, 45.032969953999974)\n","(x_q25, x_q75)=(58.675000000000004, 74.6), (y_q25, y_q75)=(33.2, 54.4)\n","x_iqr=15.92499999999999, y_iqr=21.199999999999996\n","x_ac_range=(10.900000000000034, 122.37499999999997), y_ac_range=(-30.399999999999984, 117.99999999999999)\n","Iteration 10000, Cost: 183.3784912572043\n","Iteration 20000, Cost: 180.652333721648\n","Iteration 30000, Cost: 178.06078560867513\n","Iteration 40000, Cost: 175.59720029700932\n","Iteration 50000, Cost: 173.25525935617634\n","Iteration 60000, Cost: 171.02895634139853\n","Iteration 70000, Cost: 168.91258138864902\n","Iteration 80000, Cost: 166.90070657035835\n","Iteration 90000, Cost: 164.98817197421357\n","Iteration 100000, Cost: 163.17007246934457\n","Iteration 110000, Cost: 161.441745125959\n","Iteration 120000, Cost: 159.79875725615858\n","Iteration 130000, Cost: 158.23689504526416\n","Iteration 140000, Cost: 156.7521527444941\n","Iteration 150000, Cost: 155.34072239727604\n","Iteration 160000, Cost: 153.99898407284357\n","Iteration 170000, Cost: 152.7234965820695\n","Iteration 180000, Cost: 151.51098865172472\n","Iteration 190000, Cost: 150.35835053452473\n","Iteration 200000, Cost: 149.26262603345083\n","Iteration 210000, Cost: 148.22100491988311\n","Iteration 220000, Cost: 147.23081572610747\n","Iteration 230000, Cost: 146.28951889370532\n","Iteration 240000, Cost: 145.3947002602573\n","Iteration 250000, Cost: 144.54406486765373\n","Iteration 260000, Cost: 143.7354310761329\n","Iteration 270000, Cost: 142.96672496895172\n","Iteration 280000, Cost: 142.2359750333353\n","Iteration 290000, Cost: 141.54130710406884\n","Iteration 300000, Cost: 140.88093955675706\n","Iteration 310000, Cost: 140.25317873842997\n","Iteration 320000, Cost: 139.65641462376837\n","Iteration 330000, Cost: 139.08911668581476\n","Iteration 340000, Cost: 138.54982997057536\n","Iteration 350000, Cost: 138.0371713654479\n","Iteration 360000, Cost: 137.549826051902\n","Iteration 370000, Cost: 137.08654413331743\n","Iteration 380000, Cost: 136.64613742932931\n","Iteration 390000, Cost: 136.22747642845897\n","Iteration 400000, Cost: 135.82948739121483\n","Iteration 410000, Cost: 135.4511495962349\n","Iteration 420000, Cost: 135.0914927224052\n","Iteration 430000, Cost: 134.74959436024278\n","Iteration 440000, Cost: 134.4245776461588\n","Iteration 450000, Cost: 134.11560901353434\n","Iteration 460000, Cost: 133.82189605484294\n","Iteration 470000, Cost: 133.54268548933436\n","Iteration 480000, Cost: 133.2772612310676\n","Iteration 490000, Cost: 133.02494255234055\n","Iteration 500000, Cost: 132.78508233780227\n","Iteration 510000, Cost: 132.55706542477336\n","Iteration 520000, Cost: 132.34030702551607\n","Iteration 530000, Cost: 132.13425122740827\n","Iteration 540000, Cost: 131.93836956717377\n","Iteration 550000, Cost: 131.7521596755148\n","Iteration 560000, Cost: 131.57514398866545\n","Iteration 570000, Cost: 131.4068685235686\n","Iteration 580000, Cost: 131.2469017135279\n","Iteration 590000, Cost: 131.09483330135407\n","Iteration 600000, Cost: 130.95027328716276\n","Iteration 610000, Cost: 130.81285092812857\n","Iteration 620000, Cost: 130.68221378762675\n","Iteration 630000, Cost: 130.55802683132666\n","Iteration 640000, Cost: 130.4399715679173\n","Iteration 650000, Cost: 130.32774523226146\n","Iteration 660000, Cost: 130.22106000888223\n","Iteration 670000, Cost: 130.119642293793\n","Iteration 680000, Cost: 130.0232319927752\n","Iteration 690000, Cost: 129.9315818543054\n","Iteration 700000, Cost: 129.84445683542006\n","validation_mape=np.float64(0.19377499946845542)\n"]}],"source":["# TODO\n","\n","# (1) Split data\n","training_data, validation_data = SplitData(training_datalist, 0.85)\n","\n","# (2) Preprocess data\n","training_data = PreprocessDataBasic(training_data)\n","validation_data = PreprocessDataBasic(validation_data)\n","validation_data_x = validation_data[:, 0]\n","validation_data_y = validation_data[:, 1]\n","\n","# (3) Train regression model\n","w = RegressionBasic(training_data)\n","\n","# (4) Predict validation dataset's answer, calculate MAPE comparing to the ground truth\n","predicted_res = MakePredictionBasic(w, validation_data_x)\n","validation_mape = np.sum(np.absolute(np.divide(np.subtract(validation_data_y, predicted_res), validation_data_y))) / len(validation_data)\n","print(f\"{validation_mape=}\")\n","\n","# (5) Make prediction of testing dataset and store the values in output_datalist\n","output_datalist = MakePredictionBasic(w, testing_datalist[:, 0])"]},{"cell_type":"markdown","metadata":{"id":"RW3NrFmGEEiG"},"source":["### *Write the Output File*\n","\n","Write the prediction to output csv and upload the file to Kaggle\n","> Format: 'Id', 'gripForce'\n"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"Mo7rdhx0EFLn"},"outputs":[],"source":["# Assume that output_datalist is a list (or 1d array) with length = 100\n","\n","with open(output_dataroot, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow([\"Id\", \"gripForce\"])\n","    for i in range(len(output_datalist)):\n","        writer.writerow([i, output_datalist[i]])"]},{"cell_type":"markdown","metadata":{"id":"V1O2l8d2E3he"},"source":["# 2. Advanced Part (45%)\n","In the second part, you need to implement regression differently from the basic part to improve your grip force predictions. You must use more than two features.\n","\n","You can choose either matrix inversion or gradient descent for this part\n","\n","We have provided `lab1_advanced_training.csv` for your training\n","\n","> Notice: Be cautious of the \"gender\" attribute, as it is represented by \"F\"/\"M\" rather than a numerical value.\n","\n","Please save the prediction result in a CSV file and submit it to Kaggle"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import csv\n","import math\n","import random"]},{"cell_type":"code","execution_count":102,"metadata":{"id":"Us1kieIvcucL"},"outputs":[],"source":["training_dataroot = 'lab1_advanced_training.csv' # Training data file file named as 'lab1_advanced_training.csv'\n","testing_dataroot = 'lab1_advanced_testing.csv'   # Testing data file named as 'lab1_advanced_testing.csv'\n","output_dataroot = 'lab1_advanced.csv' # Output file will be named as 'lab1_advanced.csv'\n","\n","training_datalist =  [] # Training datalist, saved as numpy array\n","testing_datalist =  [] # Testing datalist, saved as numpy array\n","\n","output_datalist =  [] # Your prediction, should be a list with 3000 elements"]},{"cell_type":"code","execution_count":103,"metadata":{"id":"zAmzFZM-dCkG"},"outputs":[],"source":["# Read input csv to datalist\n","with open(training_dataroot, newline='') as csvfile:\n","  training_datalist = pd.read_csv(training_dataroot).to_numpy()\n","\n","with open(testing_dataroot, newline='') as csvfile:\n","  testing_datalist = pd.read_csv(testing_dataroot).to_numpy()"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["def SplitDataAdvance(data, split_ratio, random_seed=-1):\n","    \"\"\"\n","    Splits the given dataset into training and validation sets based on the specified split ratio.\n","\n","    Parameters:\n","    - data (numpy.ndarray): The dataset to be split. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n","    - split_ratio (float): The ratio of the data to be used for training. For example, a value of 0.8 means 80% of the data will be used for training and the remaining 20% for validation.\n","\n","    Returns:\n","    - training_data (numpy.ndarray): The portion of the dataset used for training.\n","    - validation_data (numpy.ndarray): The portion of the dataset used for validation.\n","\n","    \"\"\"\n","    training_data = []\n","    validation_data = []\n","\n","    if random_seed != -1:\n","        np.random.default_rng(seed=random_seed).shuffle(data)\n","\n","    data_size = len(data)\n","    training_data = data[: math.floor(data_size * split_ratio)]\n","    validation_data = data[math.floor(data_size * split_ratio) :]\n","\n","    return training_data, validation_data"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["def PreprocessDataAdvance(data, drop=False, ignore_last=False):\n","    \"\"\"\n","    Preprocess the given dataset and return the result.\n","\n","    Parameters:\n","    - data (numpy.ndarray): The dataset to preprocess. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n","\n","    Returns:\n","    - preprocessedData (numpy.ndarray): Preprocessed data.\n","    \"\"\"\n","    data_copy = data\n","\n","    # encode gender\n","    for i in data_copy:\n","        if i[1] == 'M':\n","            i[1] = 1\n","        elif i[1] == 'F':\n","            i[1] = 2\n","    # print(f\"{data=}\")\n","\n","    data_removed_nan = np.array([i for i in data_copy if not np.any(pd.isnull(i))])    # 計算平均值時，如果任何一列有 nan 就直接排除\n","    # print(f\"{data_removed_nan=}\")\n","\n","    mean_val = np.mean(data_removed_nan, axis=0)\n","    q75_val = np.percentile(data_removed_nan, [75], axis=0)\n","    q25_val = np.percentile(data_removed_nan, [25], axis=0)\n","    iqr_val = q75_val - q25_val\n","    ac_range = np.dstack((q25_val - 1.5 * iqr_val, q75_val + 1.5 * iqr_val))[0]\n","    # print(f\"{ac_range=}\")\n","    # print(f\"{mean_val=}\")\n","\n","    res = []\n","    d = len(data_copy[0])\n","    for i in data_copy:\n","        # print(f\"{i=}\")\n","        # print(f\"{pd.isnull(i)=}\")\n","        # print(f\"{((ac_range[:, 0] >= i) | (i >= ac_range[:, 1]))}\")\n","\n","        condition_match = pd.isnull(i) | ((ac_range[:, 0] >= i) | (i >= ac_range[:, 1])) | (i <= 0)\n","        # condition_match = condition_match | (i == 0)\n","        miss_count = np.count_nonzero(condition_match)\n","\n","        # print(f\"{condition_match=}, {miss_count=}\")\n","        \n","        if drop and miss_count >= 2:\n","            continue\n","        else:\n","            insert_idx = np.where(condition_match == True)\n","            for idx in insert_idx:\n","                i[idx] = mean_val[idx]\n","\n","            res.append(i)\n","\n","    res = np.array(res).astype(float)\n","\n","    # print(res[7])\n","\n","    if ignore_last:\n","        res_x = res[:, :-1]\n","\n","        origin_max = np.max(res_x, axis=0)\n","        origin_min = np.min(res_x, axis=0)\n","\n","        res_x = (res_x - origin_min) / (origin_max - origin_min)\n","        res_x = np.column_stack((res_x, res[:, -1]))\n","    else:\n","        res_x = res\n","\n","        origin_max = np.max(res_x, axis=0)\n","        origin_min = np.min(res_x, axis=0)\n","\n","        res_x = (res_x - origin_min) / (origin_max - origin_min)\n","\n","    # print(f\"{origin_max}, {origin_min}\")\n","    # print(f\"{res_x[2]}\")\n","\n","    return res_x\n","\n","# tmp = PreprocessDataAdvance(training_datalist)"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["def maep(y, y_hat):\n","    # print(f\"{y=}\")\n","    for idx, i in enumerate(y):\n","        # print(f\"{idx=} {i=}\")\n","        if i == 0:\n","            print(f\"OAO {idx}, {i=}\")\n","    maep_val = np.sum(np.abs((y - y_hat) / y)) / len(y)\n","\n","    return maep_val"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["def MakePredictionAdvance(w, test_dataset, degree):\n","    \"\"\"\n","    Predicts the output for a given test dataset using a regression model.\n","\n","    Parameters:\n","    - w (numpy.ndarray): The coefficients of the model, where each element corresponds to\n","                               a coefficient for the respective power of the independent variable.\n","    - test_dataset (numpy.ndarray): A 1D array containing the input values (independent variable)\n","                                          for which predictions are to be made.\n","\n","    Returns:\n","    - list/numpy.ndarray: A list or 1d array of predicted values corresponding to each input value in the test dataset.\n","    \"\"\"\n","    # degree = 3  # For example, quadratic regression\n","\n","    X = test_dataset\n","\n","    # Add polynomial features to X\n","    X_poly = np.ones((X.shape[0], 1))  # Add intercept term (column of ones)\n","    X_poly = np.hstack((X_poly, X))\n","    for d in range(2, degree + 1):\n","        X_poly = np.hstack((X_poly, X**d))  # Add x^d terms to feature matrix\n","\n","    # print(f\"{X_poly.shape=}, {w=}\")\n","\n","    prediction = X_poly @ w\n","\n","    return prediction"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["def RegressionAdvance(\n","    dataset,\n","    maep_record,\n","    epoch=1_0000,\n","    l_rate=1e-8,\n","    degree=3,\n","    batch_size=2500,\n","    random_sgd=False,\n","    random_seed=9582463,\n","    lambda1=0.6,\n","    lambda2=0.3,\n","):\n","    \"\"\"\n","    Performs regression on the given dataset and return the coefficients.\n","\n","    Parameters:\n","    - dataset (numpy.ndarray): A 2D array where each row represents a data point.\n","\n","    Returns:\n","    - w (numpy.ndarray): The coefficients of the regression model. For example, y = w[0] + w[1] * x + w[2] * x^2 + ...\n","    \"\"\"\n","\n","    def gen_x_poly(random_seed, batch_size):\n","        if random_sgd:\n","            random_sample = np.random.default_rng(seed=random_seed).choice(\n","                dataset, batch_size\n","            )\n","            random_X = random_sample[:, :-1]\n","            random_y = random_sample[:, -1]\n","        else:\n","            random_X = dataset[:, :-1]\n","            random_y = dataset[:, -1]\n","\n","        # Add polynomial features to X\n","        res_poly = np.ones(\n","            (random_X.shape[0], 1)\n","        )  # Add intercept term (column of ones)\n","        res_poly = np.hstack((res_poly, random_X))\n","        for d in range(2, degree + 1):\n","            res_poly = np.hstack(\n","                (res_poly, random_X**d)\n","            )  # Add x^d terms to feature matrix\n","\n","        return res_poly, random_y\n","\n","    # degree = 3  # For example, quadratic regression\n","    X_poly, y = gen_x_poly(random_seed, batch_size)\n","    print(f\"{X_poly.shape=}\")\n","    # print(f\"{np.sum(y)=}\")\n","\n","    # Initialize coefficients (weights) to zero\n","    # Number of features (including intercept and polynomial terms)\n","    num_dimensions = X_poly.shape[1]\n","    w = np.zeros(num_dimensions)  # shihtl> 這個 w 就是我們要解的東西\n","    print(f\"{num_dimensions=}, {w=}\")\n","\n","    mt = vt = 0\n","    beta1 = 0.9\n","    beta2 = 0.999\n","    epsilon = 1e-8\n","\n","    least_maep = 1e4\n","    least_maep_w = -1\n","\n","    # Gradient Descent\n","    m = len(y)  # Number of data points\n","    for iteration in range(1, epoch + 1):\n","        if random_sgd:\n","            X_poly, y = gen_x_poly(random_seed, batch_size)\n","            # print(f\"{X_poly.shape=}\")\n","            random_seed += 30\n","\n","        # print(f\"{X_poly.shape=}, {w.shape=}\")\n","        y_hat = X_poly @ w\n","        d = y - y_hat\n","        # print(f\"{d=}, {y_hat=}, {len(y)=}\")\n","\n","        g = -2 * ((np.transpose(d) @ X_poly) / m)\n","        # g = -2 * (np.matmul(np.transpose(d), X_poly))\n","\n","        regularization1 = (lambda1 / m / 2) * np.sign(g)\n","        regularization2 = (lambda2 / m) * w\n","\n","        reg = g + regularization1\n","\n","        mt = beta1 * mt + (1 - beta1) * reg\n","        vt = beta2 * vt + (1 - beta2) * np.dot(reg, reg)\n","        mt_hat = mt / (1 - pow(beta1, iteration))\n","        vt_hat = vt / (1 - pow(beta2, iteration))\n","        w = w - (l_rate / (math.sqrt(vt_hat) + epsilon) * mt_hat)\n","\n","        # print(f\"{w=}\")\n","        # input()\n","\n","        # w = w - learning_rate * (g + regularization2)\n","\n","        if iteration % 100 == 0:\n","            mse = np.sum((y - y_hat) ** 2) / m\n","            valid_pred = MakePredictionAdvance(w, validation_data[:, :-1], degree)\n","            maep_val = maep(validation_data[:, -1], valid_pred)\n","            maep_record.append(maep_val)\n","            print(f\"Iteration {iteration}, MSE: {mse}, MAEP: {maep_val}\")\n","\n","        valid_pred = MakePredictionAdvance(w, validation_data[:, :-1], degree)\n","        maep_val = maep(validation_data[:, -1], valid_pred)\n","        if maep_val < least_maep:\n","            least_maep = maep_val\n","            least_maep_w = w\n","        if maep_val > 1e4:\n","            return (least_maep_w, w) if least_maep_w != -1 else (w, w)\n","        # cost = np.sum((y - y_hat) ** 2) / len(y)\n","        # print(f\"Iteration {iteration}, Cost: {cost}\\n\\n\")\n","\n","    return (least_maep_w, w)"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["# tmp_data = PreprocessDataAdvance(training_datalist[:, [0,1,4,7]])\n","# dataf = pd.DataFrame(tmp_data).convert_dtypes(int)\n","# dataf.corr()\n","# # pd.plotting.scatter_matrix(dataf, figsize=(16, 9), alpha=0.7)\n","# # plt.show()"]},{"cell_type":"code","execution_count":110,"metadata":{"id":"NSMzOXFAo2P0"},"outputs":[{"name":"stdout","output_type":"stream","text":["=====143106398_0.01_2000_0.3_3=====\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\X6959\\AppData\\Local\\Temp\\ipykernel_12036\\3652103673.py:39: RuntimeWarning: invalid value encountered in greater_equal\n","  condition_match = pd.isnull(i) | ((ac_range[:, 0] >= i) | (i >= ac_range[:, 1])) | (i <= 0)\n","C:\\Users\\X6959\\AppData\\Local\\Temp\\ipykernel_12036\\3652103673.py:39: RuntimeWarning: invalid value encountered in less_equal\n","  condition_match = pd.isnull(i) | ((ac_range[:, 0] >= i) | (i >= ac_range[:, 1])) | (i <= 0)\n"]},{"name":"stdout","output_type":"stream","text":["training_data=array([[ 0.39534884,  1.        ,  0.57438017, ...,  0.13559322,\n","         0.07407407, 28.1       ],\n","       [ 0.95348837,  0.        ,  0.73140496, ...,  0.74576271,\n","         0.7037037 , 48.6       ],\n","       [ 0.11627907,  0.        ,  0.73760331, ...,  0.61016949,\n","         0.79012346, 62.3       ],\n","       ...,\n","       [ 0.06976744,  0.        ,  0.54752066, ...,  0.3220339 ,\n","         0.39506173, 50.8       ],\n","       [ 0.65116279,  1.        ,  0.39049587, ...,  0.23728814,\n","         0.35802469, 37.1       ],\n","       [ 0.88372093,  1.        ,  0.41322314, ...,  0.52542373,\n","         0.38271605, 30.4       ]])\n","X_poly.shape=(3000, 22)\n","num_dimensions=22, w=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0.])\n","Iteration 100, MSE: 1977.7091773981724, MAEP: 0.954205838988232\n","Iteration 200, MSE: 1829.716012066034, MAEP: 0.9095992792944965\n","Iteration 300, MSE: 1691.3320814549643, MAEP: 0.8660857694204608\n","Iteration 400, MSE: 1562.0922689181045, MAEP: 0.823600751443018\n","Iteration 500, MSE: 1441.5503487314625, MAEP: 0.7821381643325286\n","Iteration 600, MSE: 1329.2776803439358, MAEP: 0.7419821841682955\n","Iteration 700, MSE: 1224.8619951285525, MAEP: 0.7029058314626518\n","Iteration 800, MSE: 1127.906258230367, MAEP: 0.6649586220582561\n","Iteration 900, MSE: 1038.0276006289716, MAEP: 0.6283114563419657\n","Iteration 1000, MSE: 954.8563217633013, MAEP: 0.5929276349397751\n","Iteration 1100, MSE: 878.0349647770979, MAEP: 0.5589153067106787\n","Iteration 1200, MSE: 807.217467422045, MAEP: 0.5266835132499301\n","Iteration 1300, MSE: 742.068392417716, MAEP: 0.49655967852169486\n","Iteration 1400, MSE: 682.2622417210763, MAEP: 0.4685998450802227\n","Iteration 1500, MSE: 627.4828597074701, MAEP: 0.4428861521625652\n","Iteration 1600, MSE: 577.4229306758506, MAEP: 0.41947940133190403\n","Iteration 1700, MSE: 531.7835763080992, MAEP: 0.3984682987473347\n","Iteration 1800, MSE: 490.2740586640427, MAEP: 0.379733094984726\n","Iteration 1900, MSE: 452.61159389497027, MAEP: 0.3630718176551678\n","Iteration 2000, MSE: 418.5212810135376, MAEP: 0.3483098420660617\n","Iteration 2100, MSE: 387.73614866628714, MAEP: 0.3352034869671984\n","Iteration 2200, MSE: 359.99732081960656, MAEP: 0.32370752687335796\n","Iteration 2300, MSE: 335.0542973161408, MAEP: 0.31362482639425954\n","Iteration 2400, MSE: 312.66534365698294, MAEP: 0.3047189246584505\n","Iteration 2500, MSE: 292.59800380766006, MAEP: 0.29695905188696115\n","Iteration 2600, MSE: 274.62967566358105, MAEP: 0.2901768675820732\n","Iteration 2700, MSE: 258.5482604912204, MAEP: 0.2841440221755981\n","Iteration 2800, MSE: 244.15285493760152, MAEP: 0.2787216143061554\n","Iteration 2900, MSE: 231.25445150560984, MAEP: 0.27383568080301895\n","Iteration 3000, MSE: 219.67661529917962, MAEP: 0.26937513751392017\n","Iteration 3100, MSE: 209.2560940695171, MAEP: 0.26519794187250767\n","Iteration 3200, MSE: 199.84332067852188, MAEP: 0.26123996522392123\n","Iteration 3300, MSE: 191.30276929535927, MAEP: 0.2574324515126156\n","Iteration 3400, MSE: 183.51312524735977, MAEP: 0.25376080415221247\n","Iteration 3500, MSE: 176.36724958216251, MAEP: 0.2501615017325414\n","Iteration 3600, MSE: 169.77191207647488, MAEP: 0.24660748594997164\n","Iteration 3700, MSE: 163.64728721673654, MAEP: 0.2430537652418743\n","Iteration 3800, MSE: 157.92623063077744, MAEP: 0.2395085083456616\n","Iteration 3900, MSE: 152.55334754259155, MAEP: 0.23597391467038695\n","Iteration 4000, MSE: 147.4838923319577, MAEP: 0.23243486209911535\n","Iteration 4100, MSE: 142.6825449537124, MAEP: 0.2289009117866894\n","Iteration 4200, MSE: 138.12211360574517, MAEP: 0.22537679563271124\n","Iteration 4300, MSE: 133.78221606367407, MAEP: 0.22186054891416032\n","Iteration 4400, MSE: 129.647993284586, MAEP: 0.21836330816027552\n","Iteration 4500, MSE: 125.70889219925309, MAEP: 0.2148899541098358\n","Iteration 4600, MSE: 121.95755820753266, MAEP: 0.2114422025580889\n","Iteration 4700, MSE: 118.38886080144133, MAEP: 0.2080679073145248\n","Iteration 4800, MSE: 114.99906374806166, MAEP: 0.204756394737016\n","Iteration 4900, MSE: 111.7851462333299, MAEP: 0.2015453406062774\n","Iteration 5000, MSE: 108.74427081582174, MAEP: 0.19844898541315764\n","Iteration 5100, MSE: 105.8733832160561, MAEP: 0.19545576248614804\n","Iteration 5200, MSE: 103.16893462247786, MAEP: 0.19257919108347538\n","Iteration 5300, MSE: 100.62670375876691, MAEP: 0.18985154836770793\n","Iteration 5400, MSE: 98.2417048311317, MAEP: 0.18726792720206745\n","Iteration 5500, MSE: 96.00816410952768, MAEP: 0.18482976591810013\n","Iteration 5600, MSE: 93.91955023354947, MAEP: 0.18251146283155328\n","Iteration 5700, MSE: 91.96864544854627, MAEP: 0.18034731041625873\n","Iteration 5800, MSE: 90.14764698078936, MAEP: 0.17831199508500692\n","Iteration 5900, MSE: 88.44828954757858, MAEP: 0.17637560102593644\n","Iteration 6000, MSE: 86.86198091575481, MAEP: 0.17454453548994647\n","Iteration 6100, MSE: 85.37994547885182, MAEP: 0.17282018811783179\n","Iteration 6200, MSE: 83.9933702251931, MAEP: 0.17120791302045024\n","Iteration 6300, MSE: 82.69354495224123, MAEP: 0.16969158879742652\n","Iteration 6400, MSE: 81.47199489442525, MAEP: 0.1682547685615658\n","Iteration 6500, MSE: 80.32060366745058, MAEP: 0.1668844160007508\n","Iteration 6600, MSE: 79.23171706493557, MAEP: 0.1655819997988899\n","Iteration 6700, MSE: 78.1982307147242, MAEP: 0.16434427032901128\n","Iteration 6800, MSE: 77.21365618866308, MAEP: 0.1631684464600351\n","Iteration 6900, MSE: 76.27216394896574, MAEP: 0.1620486267071041\n","Iteration 7000, MSE: 75.36860446241845, MAEP: 0.16096672895071878\n","Iteration 7100, MSE: 74.4985075685663, MAEP: 0.15990987352567826\n","Iteration 7200, MSE: 73.65806186812794, MAEP: 0.15888151172291276\n","Iteration 7300, MSE: 72.84407651094435, MAEP: 0.15788236826949872\n","Iteration 7400, MSE: 72.05392905098005, MAEP: 0.15691111767291926\n","Iteration 7500, MSE: 71.28550539149293, MAEP: 0.1559632856923335\n","Iteration 7600, MSE: 70.53713102895234, MAEP: 0.1550398224043123\n","Iteration 7700, MSE: 69.80750130663782, MAEP: 0.15414372519813702\n","Iteration 7800, MSE: 69.09561349746284, MAEP: 0.15326960026153244\n","Iteration 7900, MSE: 68.40070363579036, MAEP: 0.15240957786087153\n","Iteration 8000, MSE: 67.72218993049802, MAEP: 0.15157133974870332\n","Iteration 8100, MSE: 67.05962489638259, MAEP: 0.15076064760561164\n","Iteration 8200, MSE: 66.41265872082442, MAEP: 0.149967359017843\n","Iteration 8300, MSE: 65.78100971683257, MAEP: 0.14918665608711956\n","Iteration 8400, MSE: 65.16444256065598, MAEP: 0.14842765187977297\n","Iteration 8500, MSE: 64.56275460222784, MAEP: 0.14768836330863847\n","Iteration 8600, MSE: 63.97576827000739, MAEP: 0.14696125275481442\n","Iteration 8700, MSE: 63.40332781816344, MAEP: 0.14624929839222564\n","Iteration 8800, MSE: 62.84529730006963, MAEP: 0.1455567989447076\n","Iteration 8900, MSE: 62.30156232348406, MAEP: 0.14488061565452065\n","Iteration 9000, MSE: 61.77203525527333, MAEP: 0.14422199460279836\n","Iteration 9100, MSE: 61.25665323406873, MAEP: 0.1435764511608572\n","Iteration 9200, MSE: 60.75537815771747, MAEP: 0.14294189118203993\n","Iteration 9300, MSE: 60.26819463166776, MAEP: 0.14232834290272317\n","Iteration 9400, MSE: 59.79510641216676, MAEP: 0.14172901550492675\n","Iteration 9500, MSE: 59.33613138064567, MAEP: 0.14113850590749316\n","Iteration 9600, MSE: 58.891295229062905, MAEP: 0.14056571040501964\n","Iteration 9700, MSE: 58.46062421986159, MAEP: 0.14000850195996986\n","Iteration 9800, MSE: 58.044137456460355, MAEP: 0.13947315137658986\n","Iteration 9900, MSE: 57.64183912899527, MAEP: 0.13895491927660983\n","Iteration 10000, MSE: 57.25371118952506, MAEP: 0.1384581346132568\n","Iteration 10100, MSE: 56.8797068669192, MAEP: 0.13798203802692222\n","Iteration 10200, MSE: 56.51974536124064, MAEP: 0.1375271305333396\n","Iteration 10300, MSE: 56.17370796852941, MAEP: 0.1370889502378471\n","Iteration 10400, MSE: 55.84143578774552, MAEP: 0.13667574740124416\n","Iteration 10500, MSE: 55.52272893657661, MAEP: 0.13628142827511916\n","Iteration 10600, MSE: 55.21734596245369, MAEP: 0.13590433512414352\n","Iteration 10700, MSE: 54.925009241389546, MAEP: 0.13554791402263566\n","Iteration 10800, MSE: 54.64540763832208, MAEP: 0.13521117873930602\n","Iteration 10900, MSE: 54.378200144810435, MAEP: 0.1348929414745813\n","Iteration 11000, MSE: 54.123022810523615, MAEP: 0.13459119342449266\n","Iteration 11100, MSE: 53.8794936219162, MAEP: 0.13430555936253033\n","Iteration 11200, MSE: 53.64721831074514, MAEP: 0.13403641357932738\n","Iteration 11300, MSE: 53.42579612360752, MAEP: 0.13378103284792006\n","Iteration 11400, MSE: 53.21482521062476, MAEP: 0.13353751398844724\n","Iteration 11500, MSE: 53.01390743552945, MAEP: 0.13330640828171736\n","Iteration 11600, MSE: 52.82265245655592, MAEP: 0.1330875395474468\n","Iteration 11700, MSE: 52.64068097750183, MAEP: 0.13288159070305952\n","Iteration 11800, MSE: 52.46762711835424, MAEP: 0.1326867508092358\n","Iteration 11900, MSE: 52.30313990249479, MAEP: 0.13250041808261376\n","Iteration 12000, MSE: 52.14688390059901, MAEP: 0.13232002385971658\n","Iteration 12100, MSE: 51.998539108199786, MAEP: 0.1321492946693211\n","Iteration 12200, MSE: 51.85780016320348, MAEP: 0.1319857220351541\n","Iteration 12300, MSE: 51.72437503060106, MAEP: 0.1318294228435766\n","Iteration 12400, MSE: 51.59798329388866, MAEP: 0.1316824612664774\n","Iteration 12500, MSE: 51.47835419648528, MAEP: 0.13154297560013808\n","Iteration 12600, MSE: 51.36522457237542, MAEP: 0.1314119967320935\n","Iteration 12700, MSE: 51.25833679439035, MAEP: 0.13128770601819123\n","Iteration 12800, MSE: 51.15743685228613, MAEP: 0.1311679084535563\n","Iteration 12900, MSE: 51.06227166003551, MAEP: 0.13105290598718058\n","Iteration 13000, MSE: 50.972588626148564, MAEP: 0.13094076512383238\n","Iteration 13100, MSE: 50.88813600816963, MAEP: 0.13083143339899408\n","Iteration 13200, MSE: 50.80866071226805, MAEP: 0.1307266362115421\n","Iteration 13300, MSE: 50.733906264002336, MAEP: 0.13062677356179925\n","Iteration 13400, MSE: 50.6636158685156, MAEP: 0.13053151814458352\n","Iteration 13500, MSE: 50.59753231757569, MAEP: 0.13043978526869904\n","Iteration 13600, MSE: 50.53539711402384, MAEP: 0.1303527140629919\n","Iteration 13700, MSE: 50.47695197819532, MAEP: 0.13026810080650028\n","Iteration 13800, MSE: 50.42194031248501, MAEP: 0.13018683604078163\n","Iteration 13900, MSE: 50.370107823718804, MAEP: 0.13010781443986705\n","Iteration 14000, MSE: 50.32120370489739, MAEP: 0.130031209221531\n","Iteration 14100, MSE: 50.27498113618769, MAEP: 0.1299577321835646\n","Iteration 14200, MSE: 50.23119956988067, MAEP: 0.12988764537461206\n","Iteration 14300, MSE: 50.18962545380447, MAEP: 0.12981925921760637\n","Iteration 14400, MSE: 50.15003366060731, MAEP: 0.1297536470843883\n","Iteration 14500, MSE: 50.112208325784245, MAEP: 0.12968976431607082\n","Iteration 14600, MSE: 50.07594337740824, MAEP: 0.1296279267874419\n","Iteration 14700, MSE: 50.04104329650921, MAEP: 0.12956828482524146\n","Iteration 14800, MSE: 50.007323803136075, MAEP: 0.12951022895387307\n","Iteration 14900, MSE: 49.97461248168921, MAEP: 0.12945376504200493\n","Iteration 15000, MSE: 49.94274935506523, MAEP: 0.1293993585260462\n","Iteration 15100, MSE: 49.911587177926016, MAEP: 0.12934659606318644\n","Iteration 15200, MSE: 49.8809920700635, MAEP: 0.12929535364948286\n","Iteration 15300, MSE: 49.85084441402506, MAEP: 0.12924539847843008\n","Iteration 15400, MSE: 49.82103876288245, MAEP: 0.12919606250460103\n","Iteration 15500, MSE: 49.791483467601026, MAEP: 0.12914791101641557\n","Iteration 15600, MSE: 49.76209951910192, MAEP: 0.12910106645747307\n","Iteration 15700, MSE: 49.73282502053739, MAEP: 0.1290552496361422\n","Iteration 15800, MSE: 49.703611810080226, MAEP: 0.12901008194271318\n","Iteration 15900, MSE: 49.674425080414224, MAEP: 0.12896566908583365\n","Iteration 16000, MSE: 49.645243859274224, MAEP: 0.128921438654354\n","Iteration 16100, MSE: 49.61605881351755, MAEP: 0.12887751815469997\n","Iteration 16200, MSE: 49.586871471389024, MAEP: 0.12883433804828243\n","Iteration 16300, MSE: 49.557693937554916, MAEP: 0.12879268568997715\n","Iteration 16400, MSE: 49.528546908951434, MAEP: 0.1287521013345625\n","Iteration 16500, MSE: 49.49945836323026, MAEP: 0.12871196862021236\n","Iteration 16600, MSE: 49.47046226273316, MAEP: 0.1286722922817967\n","Iteration 16700, MSE: 49.44159728647255, MAEP: 0.12863255156490144\n","Iteration 16800, MSE: 49.412905619116245, MAEP: 0.1285936139360389\n","Iteration 16900, MSE: 49.38443182005477, MAEP: 0.12855540747294225\n","Iteration 17000, MSE: 49.356221788378114, MAEP: 0.12851768567595476\n","Iteration 17100, MSE: 49.328321832507356, MAEP: 0.12848003611144795\n","Iteration 17200, MSE: 49.30077784712473, MAEP: 0.12844299996786357\n","Iteration 17300, MSE: 49.273634595322385, MAEP: 0.12840687555092706\n","Iteration 17400, MSE: 49.24693509065296, MAEP: 0.12837179448005873\n","Iteration 17500, MSE: 49.220720071908076, MAEP: 0.12833825581531733\n","Iteration 17600, MSE: 49.19502756273343, MAEP: 0.12830539574468697\n","Iteration 17700, MSE: 49.16989250830117, MAEP: 0.12827388720507685\n","Iteration 17800, MSE: 49.14534648189231, MAEP: 0.12824318285871203\n","Iteration 17900, MSE: 49.12141745512046, MAEP: 0.1282133025396139\n","Iteration 18000, MSE: 49.098129626447076, MAEP: 0.12818416230620902\n","Iteration 18100, MSE: 49.07550330346132, MAEP: 0.12815589980188088\n","Iteration 18200, MSE: 49.053554835055, MAEP: 0.12812937671661628\n","Iteration 18300, MSE: 49.032296588860945, MAEP: 0.1281033505820755\n","Iteration 18400, MSE: 49.0117369247862, MAEP: 0.1280795309583001\n","Iteration 18500, MSE: 48.9918802527865, MAEP: 0.1280576577172585\n","Iteration 18600, MSE: 48.97272723611793, MAEP: 0.128036881520839\n","Iteration 18700, MSE: 48.95427493471965, MAEP: 0.12801651344458947\n","Iteration 18800, MSE: 48.93651692367763, MAEP: 0.12799617035410366\n","Iteration 18900, MSE: 48.919443482971054, MAEP: 0.1279761181914217\n","Iteration 19000, MSE: 48.903041816919405, MAEP: 0.12795618839392592\n","Iteration 19100, MSE: 48.88729629827894, MAEP: 0.12793617753197864\n","Iteration 19200, MSE: 48.87218873248942, MAEP: 0.12791690285380766\n","Iteration 19300, MSE: 48.85769863854605, MAEP: 0.1278989186877776\n","Iteration 19400, MSE: 48.84380354311502, MAEP: 0.1278821023706752\n","Iteration 19500, MSE: 48.830479284412306, MAEP: 0.1278655998875239\n","Iteration 19600, MSE: 48.81770032221654, MAEP: 0.1278494814586088\n","Iteration 19700, MSE: 48.805440050228896, MAEP: 0.1278345872012878\n","Iteration 19800, MSE: 48.79367110684421, MAEP: 0.12782068830746926\n","Iteration 19900, MSE: 48.782365680276186, MAEP: 0.12780714572411198\n","Iteration 20000, MSE: 48.771495803909126, MAEP: 0.1277936088738831\n","valid_pred=array([32.66255879, 35.45237125, 56.83795504, ..., 49.50599325,\n","       36.65694402, 28.45000758])\n","validation_data[:, -1]=array([34.8, 34.7, 55.3, ..., 45. , 38.2, 20.4])\n","maep_val=np.float64(0.1277936088738831)\n","FINAL: w=array([ 36.62562631,   7.02212785,  -9.33198654,   3.6935509 ,\n","        24.39974907, -12.7161503 ,   1.36453875,   8.44567169,\n","        -9.16657614,  -1.59019243,   1.1301878 ,   5.47398929,\n","        -5.46153094,  -0.72118818,  -0.76965277,  -1.50026529,\n","         1.25423217,   2.51111819,  -8.2062685 ,   3.58554632,\n","        -0.13050224,  -4.63498725]), least_maep_w=array([ 36.62562631,   7.02212785,  -9.33198654,   3.6935509 ,\n","        24.39974907, -12.7161503 ,   1.36453875,   8.44567169,\n","        -9.16657614,  -1.59019243,   1.1301878 ,   5.47398929,\n","        -5.46153094,  -0.72118818,  -0.76965277,  -1.50026529,\n","         1.25423217,   2.51111819,  -8.2062685 ,   3.58554632,\n","        -0.13050224,  -4.63498725])\n","preprocessed_testing_datalist=array([[0.41860465, 0.        , 0.74403471, ..., 0.62154696, 0.56363636,\n","        0.56      ],\n","       [0.13953488, 0.        , 0.61822126, ..., 0.41160221, 0.2       ,\n","        0.6       ],\n","       [0.34883721, 1.        , 0.49457701, ..., 0.71270718, 0.45454545,\n","        0.2       ],\n","       ...,\n","       [0.        , 0.        , 0.49023861, ..., 0.47790055, 0.38181818,\n","        0.37333333],\n","       [0.13953488, 1.        , 0.39696312, ..., 0.5801105 , 0.38181818,\n","        0.38666667],\n","       [0.79069767, 0.        , 0.51626898, ..., 0.44198895, 0.4       ,\n","        0.48      ]])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# TODO\n","# (1) Split data\n","# (2) Preprocess data\n","# (3) Train regression model\n","# (4) Predict validation dataset's answer, calculate MAPE comparing to the ground truth\n","# (5) Make prediction of testing dataset and store the values in output_datalist\n","from itertools import product\n","\n","p_seed = [143106398]\n","p_learning_rate = [1e-2, 1e-4, 5e-5]\n","p_batch_size = [2000]\n","p_split_ratio = [0.3, 0.5, 0.7]\n","p_degree = [3]\n","\n","for seed, learning_rate, batch_size, split_ratio, degree in list(\n","    product(p_seed, p_learning_rate, p_batch_size, p_split_ratio, p_degree)\n",")[0:1]:\n","    print(f\"====={seed}_{learning_rate}_{batch_size}_{split_ratio}_{degree}=====\")\n","\n","    preprocessed_training_datalist = PreprocessDataAdvance(training_datalist, drop=False, ignore_last=True)\n","    training_data, validation_data = SplitDataAdvance(\n","        preprocessed_training_datalist, split_ratio, random_seed=seed\n","    )\n","\n","    print(f\"{training_data=}\")\n","\n","    maep_record = []\n","    least_maep_w, w = RegressionAdvance(\n","        training_data,\n","        maep_record,\n","        l_rate=learning_rate,\n","        epoch=2_0000,\n","        degree=degree,\n","        batch_size=batch_size,\n","        random_sgd=False,\n","        random_seed=seed,\n","    )\n","    plt.plot(maep_record)\n","\n","    valid_pred = MakePredictionAdvance(w, validation_data[:, :-1], degree)\n","    maep_val = maep(validation_data[:, -1], valid_pred)\n","    least_valid_pred = MakePredictionAdvance(least_maep_w, validation_data[:, :-1], degree)\n","    least_maep_val = maep(validation_data[:, -1], least_valid_pred)\n","    print(f\"{valid_pred=}\")\n","    print(f\"{validation_data[:, -1]=}\")\n","    print(f\"{maep_val=}\")\n","\n","    print(f\"FINAL: {w=}, {least_maep_w=}\")\n","    # w = w * (origin_max - origin_min) + origin_min\n","\n","    preprocessed_testing_datalist = PreprocessDataAdvance(testing_datalist)\n","\n","    print(f\"{preprocessed_testing_datalist=}\")\n","\n","    output_datalist = MakePredictionAdvance(w, preprocessed_testing_datalist, degree)\n","    least_output_datalist = MakePredictionAdvance(\n","        least_maep_w, preprocessed_testing_datalist, degree\n","    )\n","\n","    output_datalist = np.array(output_datalist)\n","    least_output_datalist = np.array(least_output_datalist)\n","    # output_datalist = output_datalist * (origin_max - origin_min) + origin_min\n","    # least_output_datalist = least_output_datalist * (origin_max - origin_min) + origin_min\n","\n","\n","    combination_name = f\"__lastest__{seed}_{learning_rate}_{batch_size}_{split_ratio}_{degree}__{maep_val}.csv\"\n","    least_combination_name = f\"__least__{seed}_{learning_rate}_{batch_size}_{split_ratio}_{degree}__{least_maep_val}.csv\"\n","\n","    check = input()\n","    if not check:\n","        continue\n","\n","    with open(\n","        output_dataroot + combination_name, \"w\", newline=\"\", encoding=\"utf-8\"\n","    ) as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"Id\", \"gripForce\"])\n","        for i in range(len(output_datalist)):\n","            writer.writerow([i, output_datalist[i]])\n","    with open(\n","        output_dataroot + least_combination_name, \"w\", newline=\"\", encoding=\"utf-8\"\n","    ) as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"Id\", \"gripForce\"])\n","        for i in range(len(least_output_datalist)):\n","            writer.writerow([i, least_output_datalist[i]])"]},{"cell_type":"markdown","metadata":{"id":"uVz38ASe-gGV"},"source":["# Save the Code File\n","Please save your code and submit it as an ipynb file! (**Lab1.ipynb**)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":0}
