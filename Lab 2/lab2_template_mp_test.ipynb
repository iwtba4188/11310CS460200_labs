{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1poT-tpUWgo4"
      },
      "source": [
        "Mount Google Drive (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UxG3aEmhWiV4"
      },
      "outputs": [],
      "source": [
        "# shihtl> Local\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHYT0wwCWjlN"
      },
      "source": [
        "# **Lab 2 : Decision Tree and Random Forest**\n",
        "In *lab 2*, you need to finish :\n",
        "\n",
        "1. Basic Part :\n",
        "  Implement a Decision Tree model and predict whether patients in the validation set survived.\n",
        "\n",
        "  > * Section 1: Function Implementation and Testing\n",
        "  > * Section 2: Building the Decision Tree Model\n",
        "\n",
        "\n",
        "2. Advanced Part : Build a **Random Forest** model to make predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtkN1RNQKznC"
      },
      "source": [
        "❗ **Important** ❗\n",
        "Please follow the template. Follow the instructions.\n",
        "**Do not** change the code outside this code bracket if you see one.\n",
        "```\n",
        "### START CODE HERE ###\n",
        "...\n",
        "### END CODE HERE ###\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViMa9pPp0L8U"
      },
      "source": [
        "We'll be using **pandas** frequently in this template, so we've provided a link to help you get familiar with its usage: https://pandas.pydata.org/docs/user_guide/10min.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBzqwVcaWqll"
      },
      "source": [
        "## Import Packages\n",
        "\n",
        "> Note : You **cannot** import any other packages in both basic and advanced part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eb6ccSWDWrTd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "from numpy import sqrt\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHwMS_7dWtwj"
      },
      "source": [
        "# **Basic Part** (30%)\n",
        "\n",
        "## Section 1: Function Implementation and Testing\n",
        "You will implement five functions that are necessary for building a decision tree model. After implementing each function, you must run it with the given input variables to verify its correctness. Save the results of each function to a CSV file for submission.\n",
        "> * Step 1: Calculate the Entropy\n",
        "> * Step 2: Calculate the Information Gain\n",
        "> * Step 3: Find the Best Split\n",
        "> * Step 4: Split the data into two branches\n",
        "> * Step 5: Build the decision tree\n",
        "> * Step 6: Save answers\n",
        "\n",
        "\n",
        "## Section 2: Build a Decision Tree Model and make Predictions\n",
        "After implementing the functions, you will use them to build a decision tree model and make predictions. Follow the steps below to train your model and evaluate its performance.\n",
        "> * Step 1: Split the data into training set and validation set\n",
        "> * Step 2: Train a decision tree model with the training set\n",
        "> * Step 3: Predict the cases in the validation set by using the model trained in Step 2\n",
        "> * Step 4: Calculate the f1-score of your predictions in Step 3\n",
        "> * Step 5: Save answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeEPi9tfWzx_"
      },
      "source": [
        "## Load the input data\n",
        "Let's load the input file **lab2_basic_input.csv**.\n",
        "\n",
        "> Note: you will use this input data in both section 1 and section 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TIOE-YsHW3lA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>pre_icu_los_days</th>\n",
              "      <th>glucose_apache</th>\n",
              "      <th>heart_rate_apache</th>\n",
              "      <th>resprate_apache</th>\n",
              "      <th>sodium_apache</th>\n",
              "      <th>hospital_death</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28.0</td>\n",
              "      <td>26.596278</td>\n",
              "      <td>1</td>\n",
              "      <td>173.0</td>\n",
              "      <td>79.60</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>199.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51.0</td>\n",
              "      <td>36.267895</td>\n",
              "      <td>0</td>\n",
              "      <td>180.3</td>\n",
              "      <td>117.90</td>\n",
              "      <td>0.141667</td>\n",
              "      <td>88.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>81.0</td>\n",
              "      <td>24.196007</td>\n",
              "      <td>1</td>\n",
              "      <td>162.0</td>\n",
              "      <td>63.50</td>\n",
              "      <td>1.988194</td>\n",
              "      <td>285.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>83.0</td>\n",
              "      <td>21.105377</td>\n",
              "      <td>1</td>\n",
              "      <td>162.6</td>\n",
              "      <td>55.80</td>\n",
              "      <td>0.211111</td>\n",
              "      <td>189.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>76.0</td>\n",
              "      <td>20.470093</td>\n",
              "      <td>0</td>\n",
              "      <td>167.6</td>\n",
              "      <td>57.50</td>\n",
              "      <td>14.493056</td>\n",
              "      <td>278.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>60.0</td>\n",
              "      <td>46.111111</td>\n",
              "      <td>0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>149.40</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>186.0</td>\n",
              "      <td>146.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>70.0</td>\n",
              "      <td>17.361111</td>\n",
              "      <td>1</td>\n",
              "      <td>168.0</td>\n",
              "      <td>49.00</td>\n",
              "      <td>0.156944</td>\n",
              "      <td>181.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>79.0</td>\n",
              "      <td>33.274623</td>\n",
              "      <td>0</td>\n",
              "      <td>165.1</td>\n",
              "      <td>90.70</td>\n",
              "      <td>0.004861</td>\n",
              "      <td>56.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>81.0</td>\n",
              "      <td>30.462306</td>\n",
              "      <td>0</td>\n",
              "      <td>177.8</td>\n",
              "      <td>96.30</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>113.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>54.0</td>\n",
              "      <td>25.843929</td>\n",
              "      <td>0</td>\n",
              "      <td>177.8</td>\n",
              "      <td>81.70</td>\n",
              "      <td>0.007639</td>\n",
              "      <td>112.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>54.0</td>\n",
              "      <td>35.008738</td>\n",
              "      <td>1</td>\n",
              "      <td>154.9</td>\n",
              "      <td>84.00</td>\n",
              "      <td>0.702083</td>\n",
              "      <td>89.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>55.0</td>\n",
              "      <td>28.697484</td>\n",
              "      <td>0</td>\n",
              "      <td>182.9</td>\n",
              "      <td>96.00</td>\n",
              "      <td>0.048611</td>\n",
              "      <td>88.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>65.0</td>\n",
              "      <td>15.741828</td>\n",
              "      <td>1</td>\n",
              "      <td>157.4</td>\n",
              "      <td>39.00</td>\n",
              "      <td>0.099306</td>\n",
              "      <td>92.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>68.0</td>\n",
              "      <td>27.771653</td>\n",
              "      <td>1</td>\n",
              "      <td>165.1</td>\n",
              "      <td>75.70</td>\n",
              "      <td>0.015278</td>\n",
              "      <td>211.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>81.0</td>\n",
              "      <td>21.575208</td>\n",
              "      <td>1</td>\n",
              "      <td>157.5</td>\n",
              "      <td>53.52</td>\n",
              "      <td>3.043056</td>\n",
              "      <td>90.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>67.0</td>\n",
              "      <td>30.506023</td>\n",
              "      <td>0</td>\n",
              "      <td>182.9</td>\n",
              "      <td>102.05</td>\n",
              "      <td>0.350694</td>\n",
              "      <td>107.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>77.0</td>\n",
              "      <td>25.827736</td>\n",
              "      <td>0</td>\n",
              "      <td>182.9</td>\n",
              "      <td>86.40</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>97.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>69.0</td>\n",
              "      <td>23.397612</td>\n",
              "      <td>1</td>\n",
              "      <td>165.0</td>\n",
              "      <td>63.70</td>\n",
              "      <td>0.035417</td>\n",
              "      <td>224.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>77.0</td>\n",
              "      <td>34.532872</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>99.80</td>\n",
              "      <td>0.035417</td>\n",
              "      <td>250.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>64.0</td>\n",
              "      <td>27.394313</td>\n",
              "      <td>1</td>\n",
              "      <td>167.0</td>\n",
              "      <td>76.40</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>373.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>51.0</td>\n",
              "      <td>28.219692</td>\n",
              "      <td>0</td>\n",
              "      <td>185.4</td>\n",
              "      <td>97.00</td>\n",
              "      <td>0.537500</td>\n",
              "      <td>90.0</td>\n",
              "      <td>146.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>62.0</td>\n",
              "      <td>23.640816</td>\n",
              "      <td>0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>72.40</td>\n",
              "      <td>6.063889</td>\n",
              "      <td>202.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>54.0</td>\n",
              "      <td>34.710158</td>\n",
              "      <td>1</td>\n",
              "      <td>167.6</td>\n",
              "      <td>97.50</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>205.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>74.0</td>\n",
              "      <td>33.651380</td>\n",
              "      <td>0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>98.40</td>\n",
              "      <td>1.059722</td>\n",
              "      <td>184.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>52.0</td>\n",
              "      <td>20.402893</td>\n",
              "      <td>0</td>\n",
              "      <td>176.0</td>\n",
              "      <td>63.20</td>\n",
              "      <td>13.668750</td>\n",
              "      <td>105.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>70.0</td>\n",
              "      <td>28.686787</td>\n",
              "      <td>1</td>\n",
              "      <td>170.2</td>\n",
              "      <td>83.10</td>\n",
              "      <td>0.167361</td>\n",
              "      <td>201.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>78.0</td>\n",
              "      <td>21.250000</td>\n",
              "      <td>1</td>\n",
              "      <td>160.0</td>\n",
              "      <td>54.40</td>\n",
              "      <td>0.271528</td>\n",
              "      <td>201.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>86.0</td>\n",
              "      <td>19.188698</td>\n",
              "      <td>0</td>\n",
              "      <td>185.6</td>\n",
              "      <td>66.10</td>\n",
              "      <td>0.786111</td>\n",
              "      <td>111.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>58.0</td>\n",
              "      <td>23.295905</td>\n",
              "      <td>1</td>\n",
              "      <td>165.1</td>\n",
              "      <td>63.50</td>\n",
              "      <td>0.001389</td>\n",
              "      <td>264.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>69.0</td>\n",
              "      <td>51.288336</td>\n",
              "      <td>1</td>\n",
              "      <td>162.6</td>\n",
              "      <td>135.60</td>\n",
              "      <td>7.174306</td>\n",
              "      <td>158.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>78.0</td>\n",
              "      <td>30.524099</td>\n",
              "      <td>0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>0.173611</td>\n",
              "      <td>230.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>46.0</td>\n",
              "      <td>47.477519</td>\n",
              "      <td>1</td>\n",
              "      <td>162.0</td>\n",
              "      <td>124.60</td>\n",
              "      <td>0.000694</td>\n",
              "      <td>598.7</td>\n",
              "      <td>39.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>63.0</td>\n",
              "      <td>28.138070</td>\n",
              "      <td>1</td>\n",
              "      <td>157.5</td>\n",
              "      <td>69.80</td>\n",
              "      <td>0.809028</td>\n",
              "      <td>105.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>32.0</td>\n",
              "      <td>20.012958</td>\n",
              "      <td>0</td>\n",
              "      <td>175.3</td>\n",
              "      <td>61.50</td>\n",
              "      <td>0.004861</td>\n",
              "      <td>573.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>76.0</td>\n",
              "      <td>31.265432</td>\n",
              "      <td>0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>101.30</td>\n",
              "      <td>0.077778</td>\n",
              "      <td>102.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>72.0</td>\n",
              "      <td>21.165166</td>\n",
              "      <td>1</td>\n",
              "      <td>152.0</td>\n",
              "      <td>48.90</td>\n",
              "      <td>0.268056</td>\n",
              "      <td>103.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>43.0</td>\n",
              "      <td>31.294766</td>\n",
              "      <td>1</td>\n",
              "      <td>165.0</td>\n",
              "      <td>85.20</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>93.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>57.0</td>\n",
              "      <td>31.757926</td>\n",
              "      <td>1</td>\n",
              "      <td>154.9</td>\n",
              "      <td>76.20</td>\n",
              "      <td>2.721528</td>\n",
              "      <td>181.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>31.0</td>\n",
              "      <td>37.300976</td>\n",
              "      <td>1</td>\n",
              "      <td>154.9</td>\n",
              "      <td>89.50</td>\n",
              "      <td>0.423611</td>\n",
              "      <td>75.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>81.0</td>\n",
              "      <td>20.989855</td>\n",
              "      <td>0</td>\n",
              "      <td>167.6</td>\n",
              "      <td>58.96</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>212.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age        bmi  gender  height  weight  pre_icu_los_days  glucose_apache  \\\n",
              "0   28.0  26.596278       1   173.0   79.60          0.000000           199.0   \n",
              "1   51.0  36.267895       0   180.3  117.90          0.141667            88.0   \n",
              "2   81.0  24.196007       1   162.0   63.50          1.988194           285.0   \n",
              "3   83.0  21.105377       1   162.6   55.80          0.211111           189.0   \n",
              "4   76.0  20.470093       0   167.6   57.50         14.493056           278.0   \n",
              "5   60.0  46.111111       0   180.0  149.40          0.027778           186.0   \n",
              "6   70.0  17.361111       1   168.0   49.00          0.156944           181.0   \n",
              "7   79.0  33.274623       0   165.1   90.70          0.004861            56.0   \n",
              "8   81.0  30.462306       0   177.8   96.30          0.002083           113.0   \n",
              "9   54.0  25.843929       0   177.8   81.70          0.007639           112.0   \n",
              "10  54.0  35.008738       1   154.9   84.00          0.702083            89.0   \n",
              "11  55.0  28.697484       0   182.9   96.00          0.048611            88.0   \n",
              "12  65.0  15.741828       1   157.4   39.00          0.099306            92.0   \n",
              "13  68.0  27.771653       1   165.1   75.70          0.015278           211.0   \n",
              "14  81.0  21.575208       1   157.5   53.52          3.043056            90.0   \n",
              "15  67.0  30.506023       0   182.9  102.05          0.350694           107.0   \n",
              "16  77.0  25.827736       0   182.9   86.40          0.712500            97.0   \n",
              "17  69.0  23.397612       1   165.0   63.70          0.035417           224.0   \n",
              "18  77.0  34.532872       1   170.0   99.80          0.035417           250.0   \n",
              "19  64.0  27.394313       1   167.0   76.40          0.011111           373.0   \n",
              "20  51.0  28.219692       0   185.4   97.00          0.537500            90.0   \n",
              "21  62.0  23.640816       0   175.0   72.40          6.063889           202.0   \n",
              "22  54.0  34.710158       1   167.6   97.50          0.044444           205.0   \n",
              "23  74.0  33.651380       0   171.0   98.40          1.059722           184.0   \n",
              "24  52.0  20.402893       0   176.0   63.20         13.668750           105.0   \n",
              "25  70.0  28.686787       1   170.2   83.10          0.167361           201.0   \n",
              "26  78.0  21.250000       1   160.0   54.40          0.271528           201.0   \n",
              "27  86.0  19.188698       0   185.6   66.10          0.786111           111.0   \n",
              "28  58.0  23.295905       1   165.1   63.50          0.001389           264.0   \n",
              "29  69.0  51.288336       1   162.6  135.60          7.174306           158.0   \n",
              "30  78.0  30.524099       0   181.0  100.00          0.173611           230.0   \n",
              "31  46.0  47.477519       1   162.0  124.60          0.000694           598.7   \n",
              "32  63.0  28.138070       1   157.5   69.80          0.809028           105.0   \n",
              "33  32.0  20.012958       0   175.3   61.50          0.004861           573.0   \n",
              "34  76.0  31.265432       0   180.0  101.30          0.077778           102.0   \n",
              "35  72.0  21.165166       1   152.0   48.90          0.268056           103.0   \n",
              "36  43.0  31.294766       1   165.0   85.20          0.083333            93.0   \n",
              "37  57.0  31.757926       1   154.9   76.20          2.721528           181.0   \n",
              "38  31.0  37.300976       1   154.9   89.50          0.423611            75.0   \n",
              "39  81.0  20.989855       0   167.6   58.96          0.131250           212.0   \n",
              "\n",
              "    heart_rate_apache  resprate_apache  sodium_apache  hospital_death  \n",
              "0                52.0             29.0          140.0               0  \n",
              "1               104.0             31.0          143.0               0  \n",
              "2               178.0              4.0          138.0               1  \n",
              "3               115.0             18.0          158.0               0  \n",
              "4                93.0              8.0          134.0               1  \n",
              "5               146.0             34.0          139.0               1  \n",
              "6               111.0             12.0          158.0               1  \n",
              "7                37.0             44.0          141.0               0  \n",
              "8                62.0              4.0          142.0               0  \n",
              "9               110.0             24.0          136.0               1  \n",
              "10              105.0             37.0          144.0               0  \n",
              "11              115.0             33.0          138.0               0  \n",
              "12               58.0              8.0          121.0               0  \n",
              "13              104.0              4.0          128.0               1  \n",
              "14               99.0              8.0          136.0               0  \n",
              "15              105.0             11.0          149.0               1  \n",
              "16               92.0             18.0          139.0               0  \n",
              "17               90.0             37.0          137.0               1  \n",
              "18              116.0             11.0          141.0               1  \n",
              "19              141.0             31.0          139.0               1  \n",
              "20              146.0             45.0          132.0               1  \n",
              "21               52.0             12.0          129.0               1  \n",
              "22              110.0             52.0          137.0               0  \n",
              "23              103.0              4.0          136.0               0  \n",
              "24              133.0             44.0          135.0               1  \n",
              "25               95.0             25.0          139.0               1  \n",
              "26              134.0             60.0          130.0               0  \n",
              "27              152.0             30.0          137.0               1  \n",
              "28              178.0             40.0          139.0               1  \n",
              "29              154.0             45.0          138.0               1  \n",
              "30               98.0              9.0          132.0               1  \n",
              "31               39.0             12.0          130.0               1  \n",
              "32               99.0              6.0          138.0               0  \n",
              "33              107.0              7.0          140.0               0  \n",
              "34              124.0              6.0          150.0               1  \n",
              "35              130.0             31.0          147.0               1  \n",
              "36              122.0             10.0          144.0               0  \n",
              "37              137.0             47.0          141.0               1  \n",
              "38              116.0             42.0          139.0               0  \n",
              "39              138.0             12.0          154.0               0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_data = pd.read_csv('lab2_basic_input.csv')\n",
        "input_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cdYaIEjW7-r"
      },
      "source": [
        "## Global attributes\n",
        "Define the global attributes\n",
        "> Note : You **cannot** modify the values of these attributes we have provided in the basic part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S4WLhABvW6Qr"
      },
      "outputs": [],
      "source": [
        "max_depth = 2\n",
        "depth = 0\n",
        "min_samples_split = 2\n",
        "n_features = input_data.shape[1] - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxs3U-K2W-ZI"
      },
      "source": [
        "> You can add your own global attributes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6x_2G6hNXA1_"
      },
      "outputs": [],
      "source": [
        "# shihtl> own global attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qvPkvOpXBgX"
      },
      "source": [
        "## Section 1: Function Implementation and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WWTmDqyXGR5"
      },
      "source": [
        "### Step 1 & 2: Calculate the Entropy and Information Gain\n",
        "In these steps, you will implement functions to calculate entropy and information gain. These metrics are crucial for determining the best way to split the dataset at each node in the decision tree.\n",
        "\n",
        "If you need some help on Entropy and Information Gain, please refer to\n",
        "* https://codingnomads.com/decision-tree-information-gain-entropy#what-is-entropy\n",
        "* https://www.mldawn.com/decision-trees-entropy/#:~:text=In%20a%20binary%20classification%20problem%2C%20when%20Entropy%20hits%200%20it,state%20of%20purity%20and%20certainty.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4j-LodsRXLd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ans_entropy =  0.9928\n"
          ]
        }
      ],
      "source": [
        "def entropy(data):\n",
        "    \"\"\"\n",
        "    This function measures the amount of uncertainty in a probability distribution\n",
        "    args:\n",
        "    * data(type: DataFrame): the data you're calculating for the entropy\n",
        "    return:\n",
        "    * entropy_value(type: float): the data's entropy\n",
        "    \"\"\"\n",
        "    p = 0  # to count the number of cases that survived\n",
        "    n = 0  # to count the number of cases that passed away\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Hint 1: what is the equation for calculating entropy?\n",
        "    # Hint 2: consider the case when p == 0 or n == 0, what should entropy be?\n",
        "    p = data[data[\"hospital_death\"] == 1].count()[\"hospital_death\"]\n",
        "    n = data[data[\"hospital_death\"] == 0].count()[\"hospital_death\"]\n",
        "\n",
        "    # shihtl> note that log2(0) = inf\n",
        "    if p == 0 or n == 0:\n",
        "        return 0\n",
        "\n",
        "    p_ratio = p / (p + n)\n",
        "    n_ratio = n / (p + n)\n",
        "\n",
        "    entropy_value = -p_ratio * math.log2(p_ratio) - n_ratio * math.log2(n_ratio)\n",
        "    entropy_value = round(entropy_value, 4)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return entropy_value\n",
        "\n",
        "\n",
        "# [Note] You have to save the value of \"ans_entropy\" into the output file\n",
        "# Please round your answer to 4 decimal place\n",
        "ans_entropy = entropy(input_data)\n",
        "print(\"ans_entropy = \", ans_entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6pZ-HjJiDWv"
      },
      "source": [
        "Expected output:\n",
        "> ans_entropy =  0.9928"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hTvkGOCdXS0H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ans_informationGain =  0.0385\n"
          ]
        }
      ],
      "source": [
        "def information_gain(data, mask):\n",
        "    \"\"\"\n",
        "    This function will calculate the information gain\n",
        "    args:\n",
        "    * data(type: DataFrame): the data you're calculating for the information gain\n",
        "    * mask(type: Series): partition information(left/right) of current input data,\n",
        "      - boolean 1(True) represents split to left subtree\n",
        "      - boolean 0(False) represents split to right subtree\n",
        "    return:\n",
        "    * ig(type: float): the information gain you can obtain by classifying the data with this given mask\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    # Hint: you should use mask to split the data into two, then recall what is the equation for calculating information gain\n",
        "    left = data.where(mask == True)\n",
        "    right = data.where(mask == False)\n",
        "\n",
        "    left_count = left.count()[\"hospital_death\"]\n",
        "    right_count = right.count()[\"hospital_death\"]\n",
        "\n",
        "    left_ratio = left_count / (left_count + right_count)\n",
        "    right_ratio = right_count / (left_count + right_count)\n",
        "\n",
        "    ig = entropy(data) - (left_ratio * entropy(left) + right_ratio * entropy(right))\n",
        "    ig = round(ig, 4)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return ig\n",
        "\n",
        "\n",
        "# [Note] You have to save the value of \"ans_informationGain\" into your output file\n",
        "# Here, let's assume that we split the input_data with 2/3 of the data in the left subtree and 1/3 in the right subtree\n",
        "# Please round your answer to 4 decimal place\n",
        "temp1 = np.zeros((int(input_data.shape[0] / 3), 1), dtype=bool)\n",
        "temp2 = np.ones(((input_data.shape[0] - int(input_data.shape[0] / 3), 1)), dtype=bool)\n",
        "temp_mask = np.concatenate((temp1, temp2))\n",
        "df_mask = pd.DataFrame(temp_mask, columns=[\"mask\"])\n",
        "ans_informationGain = information_gain(input_data, df_mask[\"mask\"])\n",
        "print(\"ans_informationGain = \", ans_informationGain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5RLtCwcieQ8"
      },
      "source": [
        "Expected output:\n",
        "> ans_informationGain = 0.0385"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXajOk9jXefG"
      },
      "source": [
        "### Step 3: Find the Best Split\n",
        "In this step, you will use the information gain calculated for each feature to find the best split. The best split is the point where the dataset is divided into two subgroups (left and right subtrees) in a way that maximizes the reduction of entropy.\n",
        "\n",
        "\n",
        "> Method: The process involves evaluating **every possible split** for each feature in the dataset. After sorting the data, you calculate potential split points by taking the **median of two consecutive values** where they differ. This median value becomes the threshold for splitting the data into two branches. The split that results in the highest information gain is selected as the best split.\n",
        "\n",
        "> Note: The method we have provided is a straightforward and basic approach. Please use this method to complete the basic part of the assignment. However, for the advanced part, you are welcome to explore and use other methods to fine-tune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oT8UOEtzXiEu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ans_ig =  0.2146\n",
            "ans_value =  99.5\n",
            "ans_name =  glucose_apache\n"
          ]
        }
      ],
      "source": [
        "def find_best_split(data, impl_part):\n",
        "    \"\"\"\n",
        "    This function will find the best split combination of data\n",
        "    args:\n",
        "    * data(type: DataFrame): the input data\n",
        "    * impl_part(type: string): 'basic' or 'advanced' to specify which implementation to use\n",
        "    return\n",
        "    * best_ig(type: float): the best information gain you obtain\n",
        "    * best_threshold(type: float): the value that splits data into 2 branches\n",
        "    * best_feature(type: string): the feature that splits data into 2 branches\n",
        "    \"\"\"\n",
        "    best_ig = -1e9\n",
        "    best_threshold = 0\n",
        "    best_feature = \"\"\n",
        "\n",
        "    if impl_part == \"basic\":\n",
        "        # Implement this part of the function using the method we provided\n",
        "        ### START CODE HERE ###\n",
        "        data_size = data.shape[0]\n",
        "        for this_feature in data.columns[:-1]:\n",
        "            # shihtl> sort by each feature\n",
        "            this_data = data.sort_values(this_feature)\n",
        "\n",
        "            # print(this_data)\n",
        "            # input()\n",
        "\n",
        "            # shihtl> traversal all entries\n",
        "            for split_idx in range(data_size - 1):\n",
        "                if (\n",
        "                    this_data.iloc[split_idx][this_feature]\n",
        "                    == this_data.iloc[split_idx + 1][this_feature]\n",
        "                ):\n",
        "                    continue\n",
        "\n",
        "                this_threshold = (\n",
        "                    this_data.iloc[split_idx][this_feature]\n",
        "                    + this_data.iloc[split_idx + 1][this_feature]\n",
        "                ) / 2\n",
        "                this_mask = data[this_feature] <= this_threshold\n",
        "                this_ig = information_gain(data, this_mask)\n",
        "\n",
        "                # print(f\"{this_ig}, {this_threshold}, {this_feature}\")\n",
        "                # shihtl> find max ig\n",
        "                if this_ig > best_ig:\n",
        "                    best_ig, best_threshold, best_feature = (\n",
        "                        this_ig,\n",
        "                        this_threshold,\n",
        "                        this_feature,\n",
        "                    )\n",
        "\n",
        "        best_ig = round(best_ig, 4)\n",
        "\n",
        "        # best_ig, best_threshold = float(best_ig), float(best_threshold)\n",
        "        ### END CODE HERE ###\n",
        "    else:\n",
        "        # You can implement another method here for the advanced part\n",
        "        ### START CODE HERE ###\n",
        "        # shihtl> 這裡預留其他方法，結果 advance 根本調用不到 QQ\n",
        "        pass\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return best_ig, best_threshold, best_feature\n",
        "\n",
        "\n",
        "# [Note] You have to save the value of \"ans_ig\", \"ans_value\", and \"ans_name\" into the output file\n",
        "# Here, let's try to find the best split for the input_data\n",
        "# Please round your answer to 4 decimal place\n",
        "ans_ig, ans_value, ans_name = find_best_split(input_data, \"basic\")\n",
        "print(\"ans_ig = \", ans_ig)\n",
        "print(\"ans_value = \", ans_value)\n",
        "print(\"ans_name = \", ans_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovN6pxbXjQE6"
      },
      "source": [
        "Expected output:\n",
        "> ans_ig =  0.2146\n",
        "\n",
        "> ans_value =  99.5\n",
        "\n",
        "> ans_name =  glucose_apache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6koURsiXwjm"
      },
      "source": [
        "### Step 4: Split into 2 branches\n",
        "\n",
        "When you are building a decision tree, after identifying the best split, you will divide the dataset into two branches: a left branch and a right branch. Each branch represents a subset of the data based on the chosen **feature** and **split point**.\n",
        "\n",
        "* The left branch will contain the data points that meet the condition of the split (e.g., values less than or equal to the split threshold).\n",
        "* The right branch will contain the remaining data points (e.g., values greater than the split threshold).\n",
        "\n",
        "This step is essential because it creates the subgroups that the decision tree will continue to split in subsequent steps. By repeatedly splitting the data into smaller and more homogenous branches, the tree becomes more capable of accurately classifying new data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0u0_dlPwX07H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ans_left =  7\n"
          ]
        }
      ],
      "source": [
        "def make_partition(data, feature, threshold):\n",
        "    \"\"\"\n",
        "    This function will split the data into 2 branches\n",
        "    args:\n",
        "    * data(type: DataFrame): the input data\n",
        "    * feature(type: string): the attribute(column name)\n",
        "    * threshold(type: float): the threshold for splitting the data\n",
        "    return:\n",
        "    * left(type: DataFrame): the divided data that matches(less than or equal to) the assigned feature's threshold\n",
        "    * right(type: DataFrame): the divided data that doesn't match the assigned feature's threshold\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    left = data[data[feature] <= threshold]\n",
        "    right = data[data[feature] > threshold]\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return left, right\n",
        "\n",
        "\n",
        "# [Note] You have to save the value of \"ans_left\" into the output file\n",
        "# Here, let's assume the best split is when we choose bmi as the feature and threshold as 21.0\n",
        "left, right = make_partition(input_data, \"bmi\", 21.0)\n",
        "ans_left = left.shape[0]\n",
        "print(\"ans_left = \", ans_left)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJjyqmgXj2GX"
      },
      "source": [
        "Expected output:\n",
        "> ans_left = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJKThK7yX8XX"
      },
      "source": [
        "### Step 5: Build the Decision Tree\n",
        "Hang in there... we are almost done with this section!\n",
        "\n",
        "Now, you need to use the above functions to complete a build_tree function.\n",
        "\n",
        "> Method:\n",
        "1.  If current depth < max_depth and the remaining number of samples > min_samples_split: continue to classify those samples\n",
        "2.  Use function *find_best_split()* to find the best split combination\n",
        "3.  If the obtained information gain is **greater than 0**: can build a deeper decision tree (add depth)\n",
        "4. Use function *make_partition()* to split the data into two parts\n",
        "5. Save the features and corresponding thresholds (starting from the root) used by the decision tree into *ans_features[]* and *ans_thresholds[]* respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZC7qkOjAYAlQ"
      },
      "outputs": [],
      "source": [
        "def build_tree(data, max_depth, min_samples_split, depth):\n",
        "    \"\"\"\n",
        "    This function will build the decision tree\n",
        "    args:\n",
        "    * data(type: DataFrame): the data you want to apply to the decision tree\n",
        "    * max_depth: the maximum depth of a decision tree\n",
        "    * min_samples_split: the minimum number of instances required to do partition\n",
        "    * depth: the height of the current decision tree\n",
        "    return:\n",
        "    * subtree: the decision tree structure including root, branch, and leaf (with the attributes and thresholds)\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    # check the condition of current depth and the remaining number of samples\n",
        "    if depth < max_depth and data.shape[0] > min_samples_split:\n",
        "        # call find_best_split() to find the best combination\n",
        "        ig, threshold, feature = find_best_split(data, \"basic\")\n",
        "\n",
        "        # check the value of information gain is greater than 0 or not\n",
        "        if ig > 0:\n",
        "            # update the depth\n",
        "            depth += 1\n",
        "\n",
        "            # call make_partition() to split the data into two parts\n",
        "            left, right = make_partition(data, feature, threshold)\n",
        "            # print(left)\n",
        "            # print(right)\n",
        "\n",
        "            # If there is no data split to the left tree OR no data split to the right tree\n",
        "            if left.empty or right.empty:\n",
        "                # return the label of the majority\n",
        "                label = int(data[\"hospital_death\"].mode().iloc[0])\n",
        "                return label\n",
        "            else:\n",
        "                question = \"{} {} {}\".format(feature, \"<=\", threshold)\n",
        "                subtree = {question: []}\n",
        "\n",
        "\n",
        "                # call function build_tree() to recursively build the left subtree and right subtree\n",
        "                left_subtree = build_tree(left, max_depth, min_samples_split, depth)\n",
        "                right_subtree = build_tree(right, max_depth, min_samples_split, depth)\n",
        "\n",
        "                if left_subtree == right_subtree:\n",
        "                    subtree = left_subtree\n",
        "                else:\n",
        "                    ans_features.append(feature)\n",
        "                    ans_thresholds.append(int(threshold))\n",
        "                    \n",
        "                    subtree[question].append(left_subtree)\n",
        "                    subtree[question].append(right_subtree)\n",
        "        else:\n",
        "            # return the label of the majority\n",
        "            label = int(data[\"hospital_death\"].mode().iloc[0])\n",
        "            return label\n",
        "    else:\n",
        "        # return the label of the majority\n",
        "        label = int(data[\"hospital_death\"].mode().iloc[0])\n",
        "        return label\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return subtree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7URY3IMkULl"
      },
      "source": [
        "An example of the output from *build_tree()*\n",
        "```\n",
        "{'bmi <= 33.5': [1, {'age <= 68.5': [0, 1]}]}\n",
        "```\n",
        "Therefore,\n",
        "```\n",
        "ans_features = ['bmi', 'age']\n",
        "ans_thresholds = [33.5, 68.5]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dAuaqjhuYQSi"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'glucose_apache <= 99.5': [{'height <= 184.15': [0, 1]}, 1]}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Here, let's build a decision tree using the input_data\n",
        "\n",
        "ans_features = []\n",
        "ans_thresholds = []\n",
        "\n",
        "decisionTree = build_tree(input_data, max_depth, min_samples_split, depth)\n",
        "decisionTree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD6A_jADyTjw"
      },
      "source": [
        "Expected output:\n",
        "> decisionTree = {'glucose_apache <= 99.5': [{'height <= 184.15': [0, 1]}, 1]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oTdQ3vVkYYEQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['height', 'glucose_apache']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# [Note] You have to save the features in the \"decisionTree\" structure into the output file\n",
        "ans_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZO17E9vkbi4"
      },
      "source": [
        "Expected output:\n",
        "> ans_features = ['height', 'glucose_apache']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BUeZh1o5YYem"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[184, 99]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# [Note] You have to save the corresponding thresholds for the features in the \"ans_features\" list into the output file\n",
        "ans_thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUPTy5RWj-1D"
      },
      "source": [
        "Expected output:\n",
        "> ans_thresholds = [184.15, 99.5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69TQQNVaYdmp"
      },
      "source": [
        "### Step 6: Save answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B9DosXRQYbDg"
      },
      "outputs": [],
      "source": [
        "basic = []\n",
        "basic.append(ans_entropy)\n",
        "basic.append(ans_informationGain)\n",
        "basic.append([ans_ig, ans_value, ans_name])\n",
        "basic.append(ans_left)\n",
        "basic.append(ans_features + ans_thresholds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpMpPwySYlUu"
      },
      "source": [
        "## Section 2: Build a Decision Tree Model\n",
        "\n",
        "Congrats! You have completed all 5 crucial functions. Now, we will use the functions above to implement a simple decision tree. You will train the decision tree using a training set and make predictions using a validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1egGj-lYn6O"
      },
      "source": [
        "### Step 1: Split data into training set and validation set\n",
        "> Note: We have split the data into training set and validation. You **cannot** change the distribution of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GAHFMO4QYpZ1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40, 11)\n",
            "(30, 11)\n",
            "(10, 11)\n"
          ]
        }
      ],
      "source": [
        "num_train = 30\n",
        "num_validation = 10\n",
        "\n",
        "training_data = input_data.iloc[:num_train]\n",
        "validation_data = input_data.iloc[-num_validation:]\n",
        "\n",
        "y_train = training_data[['hospital_death']]\n",
        "x_train = training_data.drop(['hospital_death'], axis=1)\n",
        "\n",
        "y_validation = validation_data[['hospital_death']]\n",
        "x_validation = validation_data.drop(['hospital_death'], axis=1)\n",
        "y_validation = y_validation.values.flatten()\n",
        "\n",
        "print(input_data.shape)\n",
        "print(training_data.shape)\n",
        "print(validation_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQlhfOJ0YySZ"
      },
      "source": [
        "### Step 2 to 4 : Make predictions with a decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5IHxhEjY1rN"
      },
      "source": [
        "Define the attributions of the decision tree\n",
        "> You **cannot** modify the values of these attributes in this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Vwt7BRJ1Y3hD"
      },
      "outputs": [],
      "source": [
        "max_depth = 2\n",
        "depth = 0\n",
        "min_samples_split = 2\n",
        "n_features = x_train.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-ly7gvtY5VQ"
      },
      "source": [
        "We have finished the function 'classify_data()' below, however, you can modify this function if you prefer completing it on your own way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "V4LfxAnYY6UQ"
      },
      "outputs": [],
      "source": [
        "def classify_data(instance, tree):\n",
        "    \"\"\"\n",
        "    This function will predict/classify the input instance\n",
        "    args:\n",
        "    * instance: a instance(case) to be predicted\n",
        "    return:\n",
        "    * answer: the prediction result (the classification result)\n",
        "    \"\"\"\n",
        "    equation = list(tree.keys())[0]\n",
        "    if equation.split()[1] == \"<=\":\n",
        "        temp_feature = equation.split()[0]\n",
        "        temp_threshold = equation.split()[2]\n",
        "        if instance[temp_feature] > float(temp_threshold):\n",
        "            answer = tree[equation][1]\n",
        "        else:\n",
        "            answer = tree[equation][0]\n",
        "    else:\n",
        "        if instance[equation.split()[0]] in (equation.split()[2]):\n",
        "            answer = tree[equation][0]\n",
        "        else:\n",
        "            answer = tree[equation][1]\n",
        "\n",
        "    if not isinstance(answer, dict):\n",
        "        return answer\n",
        "    else:\n",
        "        return classify_data(instance, answer)\n",
        "\n",
        "\n",
        "def make_prediction(tree, data):\n",
        "    \"\"\"\n",
        "    This function will use your pre-trained decision tree to predict the labels of all instances in data\n",
        "    args:\n",
        "    * tree: the decision tree\n",
        "    * data: the data to predict\n",
        "    return:\n",
        "    * y_prediction: the predictions\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    # [Note] You can call the function classify_data() to predict the label of each instance\n",
        "    y_prediction = []\n",
        "    for _, entry in data.iterrows():\n",
        "        y_prediction.append(classify_data(entry, tree))\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return y_prediction\n",
        "\n",
        "\n",
        "def calculate_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    This function will calculate the f1-score of the predictions\n",
        "    args:\n",
        "    * y_true: the ground truth\n",
        "    * y_pred: the predictions\n",
        "    return:\n",
        "    * score: the f1-score\n",
        "    \"\"\"\n",
        "    score = f1_score(y_true, y_pred)\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_VEmtbmtZLQJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ans_f1score =  0.4444\n"
          ]
        }
      ],
      "source": [
        "decision_tree = build_tree(training_data, max_depth, min_samples_split, depth)\n",
        "\n",
        "y_pred = make_prediction(decision_tree, x_validation)\n",
        "\n",
        "# [Note] You have to save the value of \"ans_f1score\" into your output file\n",
        "# Please round your answer to 4 decimal place\n",
        "ans_f1score = calculate_score(y_validation, y_pred)\n",
        "ans_f1score = round(ans_f1score, 4)\n",
        "print(\"ans_f1score = \", ans_f1score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUcJZRYZk4kX"
      },
      "source": [
        "Expected output:\n",
        "> ans_f1score =  0.4444"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5t58_-BwpGnY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 0, 1, 0, 0, 0, 0, 0, 1]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is just for you to check your predictions\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7NM7CvdpHzb"
      },
      "source": [
        "Expected output:\n",
        "> y_pred = [1, 1, 0, 1, 0, 0, 0, 0, 0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COjs0B5jZQ8y"
      },
      "source": [
        "### Step 5: Save answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Hijx-U2yZUAk"
      },
      "outputs": [],
      "source": [
        "basic.append(ans_f1score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9lr2gbVZUwc"
      },
      "source": [
        "## Write to Output File\n",
        "Save all of your answers into a csv file named **lab2_basic.csv**\n",
        "> Note: Please do not touch the code in this step, we have made sure this outputs the correct file format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a5_ifgVZZZKZ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ans</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.9928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.2146, 99.5, glucose_apache]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[height, glucose_apache, 184, 99]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.4444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Ans\n",
              "Id                                   \n",
              "0                              0.9928\n",
              "1                              0.0385\n",
              "2      [0.2146, 99.5, glucose_apache]\n",
              "3                                   7\n",
              "4   [height, glucose_apache, 184, 99]\n",
              "5                              0.4444"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basic_path = 'lab2_basic.csv'\n",
        "\n",
        "basic_df = pd.DataFrame({'Id': range(len(basic)), 'Ans': basic})\n",
        "basic_df.set_index('Id', inplace=True)\n",
        "basic_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OTK0JT1965qS"
      },
      "outputs": [],
      "source": [
        "basic_df.to_csv(basic_path, header = True, index = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chm8Ro6zZsWp"
      },
      "source": [
        "# **Advanced Part** (65%)\n",
        "\n",
        "In the advanced section of this lab, you will enhance your prediction capabilities by implementing a more powerful and complex machine learning model—Random Forests. Random Forests are an ensemble learning method that builds multiple decision trees and combines their outputs to improve prediction accuracy and model robustness.\n",
        "> * Step 1: Load training and testing data\n",
        "> * Step 2: Split training data into training and validation set\n",
        "> * Step 3: Build a Random Forest\n",
        "> * Step 4: Make predictions with the random forest\n",
        "> * Step 5: Write the Output File\n",
        "\n",
        "> ❗ **Important** ❗ You are allowed to create new functions to fine tune your random forest, but please make sure to **complete the functions provided**.\n",
        "\n",
        "\n",
        "\n",
        "We have attached some references if you need help:\n",
        "> https://medium.com/chung-yi/ml%E5%85%A5%E9%96%80-%E5%8D%81%E4%B8%83-%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-6afc24871857\n",
        "\n",
        "> https://www.geeksforgeeks.org/random-forest-algorithm-in-machine-learning/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kBs4D02Z0Ub"
      },
      "source": [
        "### Step 1: Load training and testing data\n",
        "First, load **lab2_advanced_training.csv**. You will use this to **train** the random forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "vLTMFK14Z4z5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>pre_icu_los_days</th>\n",
              "      <th>arf_apache</th>\n",
              "      <th>bun_apache</th>\n",
              "      <th>creatinine_apache</th>\n",
              "      <th>gcs_eyes_apache</th>\n",
              "      <th>...</th>\n",
              "      <th>temp_apache</th>\n",
              "      <th>ventilated_apache</th>\n",
              "      <th>wbc_apache</th>\n",
              "      <th>apache_4a_hospital_death_prob</th>\n",
              "      <th>apache_4a_icu_death_prob</th>\n",
              "      <th>aids</th>\n",
              "      <th>cirrhosis</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "      <th>leukemia</th>\n",
              "      <th>hospital_death</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>79.0</td>\n",
              "      <td>25.616497</td>\n",
              "      <td>1</td>\n",
              "      <td>168.0</td>\n",
              "      <td>72.3</td>\n",
              "      <td>0.305556</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.92</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>36.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.20</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43.0</td>\n",
              "      <td>23.494409</td>\n",
              "      <td>0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>68.7</td>\n",
              "      <td>0.011806</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>39.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.20</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>62.0</td>\n",
              "      <td>29.145882</td>\n",
              "      <td>0</td>\n",
              "      <td>182.9</td>\n",
              "      <td>97.5</td>\n",
              "      <td>0.006250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>3.59</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.20</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>72.0</td>\n",
              "      <td>41.183318</td>\n",
              "      <td>1</td>\n",
              "      <td>170.2</td>\n",
              "      <td>119.3</td>\n",
              "      <td>1.945139</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>2.25</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>37.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.40</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>87.0</td>\n",
              "      <td>22.914211</td>\n",
              "      <td>0</td>\n",
              "      <td>170.1</td>\n",
              "      <td>66.3</td>\n",
              "      <td>0.085417</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.60</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>36.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.50</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8495</th>\n",
              "      <td>55.0</td>\n",
              "      <td>33.201250</td>\n",
              "      <td>0</td>\n",
              "      <td>165.1</td>\n",
              "      <td>90.5</td>\n",
              "      <td>0.052083</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.09</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>37.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.60</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8496</th>\n",
              "      <td>87.0</td>\n",
              "      <td>29.756001</td>\n",
              "      <td>1</td>\n",
              "      <td>142.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.014583</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.80</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>35.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.70</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8497</th>\n",
              "      <td>80.0</td>\n",
              "      <td>17.630854</td>\n",
              "      <td>1</td>\n",
              "      <td>165.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.102778</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.90</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>35.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>45.80</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8498</th>\n",
              "      <td>74.0</td>\n",
              "      <td>19.199423</td>\n",
              "      <td>0</td>\n",
              "      <td>175.3</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.460417</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>2.19</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>33.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8499</th>\n",
              "      <td>77.0</td>\n",
              "      <td>29.412958</td>\n",
              "      <td>1</td>\n",
              "      <td>157.0</td>\n",
              "      <td>72.5</td>\n",
              "      <td>0.538889</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.94</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>36.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8500 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age        bmi  gender  height  weight  pre_icu_los_days  arf_apache  \\\n",
              "0     79.0  25.616497       1   168.0    72.3          0.305556         0.0   \n",
              "1     43.0  23.494409       0   171.0    68.7          0.011806         0.0   \n",
              "2     62.0  29.145882       0   182.9    97.5          0.006250         0.0   \n",
              "3     72.0  41.183318       1   170.2   119.3          1.945139         0.0   \n",
              "4     87.0  22.914211       0   170.1    66.3          0.085417         0.0   \n",
              "...    ...        ...     ...     ...     ...               ...         ...   \n",
              "8495  55.0  33.201250       0   165.1    90.5          0.052083         0.0   \n",
              "8496  87.0  29.756001       1   142.0    60.0          0.014583         0.0   \n",
              "8497  80.0  17.630854       1   165.0    48.0          0.102778         0.0   \n",
              "8498  74.0  19.199423       0   175.3    59.0          0.460417         0.0   \n",
              "8499  77.0  29.412958       1   157.0    72.5          0.538889         0.0   \n",
              "\n",
              "      bun_apache  creatinine_apache  gcs_eyes_apache  ...  temp_apache  \\\n",
              "0           20.0               0.92              4.0  ...         36.3   \n",
              "1            9.0               0.70              1.0  ...         39.5   \n",
              "2           54.0               3.59              1.0  ...         35.0   \n",
              "3           53.0               2.25              4.0  ...         37.1   \n",
              "4           33.0               1.60              4.0  ...         36.1   \n",
              "...          ...                ...              ...  ...          ...   \n",
              "8495        15.0               1.09              4.0  ...         37.7   \n",
              "8496        17.0               0.80              4.0  ...         35.7   \n",
              "8497        30.0               0.90              3.0  ...         35.6   \n",
              "8498        39.0               2.19              1.0  ...         33.7   \n",
              "8499        22.0               0.94              4.0  ...         36.3   \n",
              "\n",
              "      ventilated_apache  wbc_apache  apache_4a_hospital_death_prob  \\\n",
              "0                   1.0        7.20                           0.28   \n",
              "1                   1.0       21.20                           0.53   \n",
              "2                   1.0       19.20                           0.62   \n",
              "3                   0.0       10.40                           0.11   \n",
              "4                   1.0       16.50                           0.16   \n",
              "...                 ...         ...                            ...   \n",
              "8495                1.0        7.60                           0.06   \n",
              "8496                0.0       11.70                           0.05   \n",
              "8497                1.0       45.80                           0.25   \n",
              "8498                1.0        3.20                           0.79   \n",
              "8499                0.0        1.93                           0.17   \n",
              "\n",
              "      apache_4a_icu_death_prob  aids  cirrhosis  diabetes_mellitus  leukemia  \\\n",
              "0                         0.07   0.0        0.0                0.0       0.0   \n",
              "1                         0.48   0.0        0.0                0.0       0.0   \n",
              "2                         0.45   0.0        0.0                0.0       0.0   \n",
              "3                         0.02   0.0        0.0                1.0       0.0   \n",
              "4                         0.08   0.0        0.0                0.0       0.0   \n",
              "...                        ...   ...        ...                ...       ...   \n",
              "8495                      0.04   0.0        0.0                0.0       0.0   \n",
              "8496                      0.02   0.0        0.0                0.0       0.0   \n",
              "8497                      0.12   0.0        0.0                0.0       0.0   \n",
              "8498                      0.72   0.0        0.0                0.0       0.0   \n",
              "8499                      0.09   0.0        0.0                1.0       0.0   \n",
              "\n",
              "      hospital_death  \n",
              "0                  0  \n",
              "1                  0  \n",
              "2                  1  \n",
              "3                  1  \n",
              "4                  0  \n",
              "...              ...  \n",
              "8495               0  \n",
              "8496               0  \n",
              "8497               1  \n",
              "8498               1  \n",
              "8499               0  \n",
              "\n",
              "[8500 rows x 30 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advanced_training_data = pd.read_csv('lab2_advanced_training.csv')\n",
        "advanced_training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjPSZoAuZ4Ry"
      },
      "source": [
        "Next, load **lab2_advanced_testing.csv**. You will make predictions on this testing data using the pre-trained random forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6k-HFk7tZ_eN"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>pre_icu_los_days</th>\n",
              "      <th>arf_apache</th>\n",
              "      <th>bun_apache</th>\n",
              "      <th>creatinine_apache</th>\n",
              "      <th>gcs_eyes_apache</th>\n",
              "      <th>...</th>\n",
              "      <th>sodium_apache</th>\n",
              "      <th>temp_apache</th>\n",
              "      <th>ventilated_apache</th>\n",
              "      <th>wbc_apache</th>\n",
              "      <th>apache_4a_hospital_death_prob</th>\n",
              "      <th>apache_4a_icu_death_prob</th>\n",
              "      <th>aids</th>\n",
              "      <th>cirrhosis</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "      <th>leukemia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>82</td>\n",
              "      <td>38.733847</td>\n",
              "      <td>1</td>\n",
              "      <td>158.23</td>\n",
              "      <td>96.82</td>\n",
              "      <td>0.232639</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "      <td>3.32</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>135</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.8</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65</td>\n",
              "      <td>22.692476</td>\n",
              "      <td>0</td>\n",
              "      <td>173.67</td>\n",
              "      <td>69.40</td>\n",
              "      <td>0.121528</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>1.40</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>133</td>\n",
              "      <td>32.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.5</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72</td>\n",
              "      <td>33.702285</td>\n",
              "      <td>0</td>\n",
              "      <td>177.47</td>\n",
              "      <td>105.70</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17</td>\n",
              "      <td>1.71</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>143</td>\n",
              "      <td>33.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>81</td>\n",
              "      <td>20.274075</td>\n",
              "      <td>0</td>\n",
              "      <td>171.74</td>\n",
              "      <td>61.10</td>\n",
              "      <td>0.664583</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>2.09</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>136</td>\n",
              "      <td>36.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>41</td>\n",
              "      <td>29.027749</td>\n",
              "      <td>1</td>\n",
              "      <td>175.75</td>\n",
              "      <td>90.00</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.41</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>149</td>\n",
              "      <td>32.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>61</td>\n",
              "      <td>22.840598</td>\n",
              "      <td>0</td>\n",
              "      <td>174.96</td>\n",
              "      <td>71.20</td>\n",
              "      <td>0.009722</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22</td>\n",
              "      <td>0.90</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>134</td>\n",
              "      <td>36.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>74</td>\n",
              "      <td>28.843833</td>\n",
              "      <td>1</td>\n",
              "      <td>169.31</td>\n",
              "      <td>82.90</td>\n",
              "      <td>0.389583</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12</td>\n",
              "      <td>0.74</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>144</td>\n",
              "      <td>36.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>68</td>\n",
              "      <td>22.744572</td>\n",
              "      <td>1</td>\n",
              "      <td>170.02</td>\n",
              "      <td>67.05</td>\n",
              "      <td>2.172917</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>1.87</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>137</td>\n",
              "      <td>35.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.7</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>55</td>\n",
              "      <td>25.356784</td>\n",
              "      <td>0</td>\n",
              "      <td>169.90</td>\n",
              "      <td>73.90</td>\n",
              "      <td>4.186111</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12</td>\n",
              "      <td>1.38</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>139</td>\n",
              "      <td>35.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>62</td>\n",
              "      <td>16.720580</td>\n",
              "      <td>1</td>\n",
              "      <td>165.40</td>\n",
              "      <td>46.00</td>\n",
              "      <td>0.157639</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11</td>\n",
              "      <td>2.80</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>135</td>\n",
              "      <td>36.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age        bmi  gender  height  weight  pre_icu_los_days  arf_apache  \\\n",
              "0     82  38.733847       1  158.23   96.82          0.232639         0.0   \n",
              "1     65  22.692476       0  173.67   69.40          0.121528         0.0   \n",
              "2     72  33.702285       0  177.47  105.70          0.143750         0.0   \n",
              "3     81  20.274075       0  171.74   61.10          0.664583         0.0   \n",
              "4     41  29.027749       1  175.75   90.00          0.004167         0.0   \n",
              "..   ...        ...     ...     ...     ...               ...         ...   \n",
              "895   61  22.840598       0  174.96   71.20          0.009722         0.0   \n",
              "896   74  28.843833       1  169.31   82.90          0.389583         0.0   \n",
              "897   68  22.744572       1  170.02   67.05          2.172917         0.0   \n",
              "898   55  25.356784       0  169.90   73.90          4.186111         0.0   \n",
              "899   62  16.720580       1  165.40   46.00          0.157639         0.0   \n",
              "\n",
              "     bun_apache  creatinine_apache  gcs_eyes_apache  ...  sodium_apache  \\\n",
              "0            50               3.32              1.0  ...            135   \n",
              "1            33               1.40              1.0  ...            133   \n",
              "2            17               1.71              1.0  ...            143   \n",
              "3            35               2.09              3.0  ...            136   \n",
              "4             3               0.41              1.0  ...            149   \n",
              "..          ...                ...              ...  ...            ...   \n",
              "895          22               0.90              4.0  ...            134   \n",
              "896          12               0.74              3.0  ...            144   \n",
              "897          36               1.87              3.0  ...            137   \n",
              "898          12               1.38              4.0  ...            139   \n",
              "899          11               2.80              4.0  ...            135   \n",
              "\n",
              "     temp_apache  ventilated_apache  wbc_apache  \\\n",
              "0           33.0                1.0        14.8   \n",
              "1           32.1                1.0        12.5   \n",
              "2           33.9                1.0        17.8   \n",
              "3           36.4                1.0         9.0   \n",
              "4           32.3                1.0        24.0   \n",
              "..           ...                ...         ...   \n",
              "895         36.5                0.0         5.2   \n",
              "896         36.4                1.0         9.9   \n",
              "897         35.8                1.0        29.7   \n",
              "898         35.9                1.0        15.3   \n",
              "899         36.2                1.0         5.8   \n",
              "\n",
              "     apache_4a_hospital_death_prob  apache_4a_icu_death_prob  aids  cirrhosis  \\\n",
              "0                             0.84                      0.71   0.0        0.0   \n",
              "1                             0.70                      0.54   0.0        0.0   \n",
              "2                             0.49                      0.29   0.0        0.0   \n",
              "3                             0.40                      0.31   0.0        0.0   \n",
              "4                             0.71                      0.64   0.0        0.0   \n",
              "..                             ...                       ...   ...        ...   \n",
              "895                           0.04                      0.03   0.0        0.0   \n",
              "896                           0.02                      0.00   0.0        0.0   \n",
              "897                           0.16                      0.11   0.0        0.0   \n",
              "898                           0.01                     -0.02   0.0        0.0   \n",
              "899                           0.08                      0.04   0.0        0.0   \n",
              "\n",
              "     diabetes_mellitus  leukemia  \n",
              "0                  0.0       0.0  \n",
              "1                  1.0       0.0  \n",
              "2                  0.0       0.0  \n",
              "3                  0.0       0.0  \n",
              "4                  0.0       0.0  \n",
              "..                 ...       ...  \n",
              "895                0.0       0.0  \n",
              "896                0.0       0.0  \n",
              "897                1.0       0.0  \n",
              "898                0.0       0.0  \n",
              "899                0.0       0.0  \n",
              "\n",
              "[900 rows x 29 columns]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advanced_testing_data = pd.read_csv('lab2_advanced_testing.csv')\n",
        "advanced_testing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMQdTAvOaIh6"
      },
      "source": [
        "### Step 2: Split training data into training and validation set (Optional)\n",
        "> You can split the training data into training and validation set, this is up to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "iMan7jJ-aKX9"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "# shihtl> 暫時用 95%\n",
        "training_data = advanced_training_data[: math.ceil(advanced_training_data.shape[0] * 0.95)]\n",
        "validation_data = advanced_training_data[math.ceil(advanced_training_data.shape[0] * 0.95) :]\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# shihtl> Turn this section into comment before hand this file in\n",
        "# shihtl> COMMENT before hand in <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(name)s | %(levelname)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %I:%M:%S\",\n",
        ")\n",
        "# shihtl> COMMENT before hand in >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY8HvRY4af3u"
      },
      "source": [
        "### Step 3: Build a Random Forest\n",
        "\n",
        "Define the attributions of the random forest\n",
        "> * You **can** modify the values of these attributes in advanced part\n",
        "> * Each tree can have different attribute values\n",
        "> * Must use function *build_tree()* to build a random forest model\n",
        "> * Must print out the *selected_datas* and *selected_features*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "xK0iM7goa2pj"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "# Define the attributes\n",
        "max_depth = 5\n",
        "depth = 0\n",
        "min_samples_split = 200\n",
        "\n",
        "# total number of trees in a random forest\n",
        "n_trees = 2\n",
        "\n",
        "# number of features to train a decision tree\n",
        "n_features = 4\n",
        "\n",
        "# the ratio to select the number of instances\n",
        "sample_size = 0.1\n",
        "n_samples = int(training_data.shape[0] * sample_size)\n",
        "\n",
        "# seed\n",
        "SEED = 48567108467\n",
        "random.seed(SEED)\n",
        "rng = np.random.default_rng(seed=SEED)\n",
        "\n",
        "# ig split attributes\n",
        "percentile_count = 10\n",
        "percentile_each = 1 / percentile_count\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_best_split_advance(data):\n",
        "    \"\"\"\n",
        "    This function will find the best split combination of data\n",
        "    args:\n",
        "    * data(type: DataFrame): the input data\n",
        "    * impl_part(type: string): 'basic' or 'advanced' to specify which implementation to use\n",
        "    return\n",
        "    * best_ig(type: float): the best information gain you obtain\n",
        "    * best_threshold(type: float): the value that splits data into 2 branches\n",
        "    * best_feature(type: string): the feature that splits data into 2 branches\n",
        "    \"\"\"\n",
        "    best_ig = -1e9\n",
        "    best_threshold = 0\n",
        "    best_feature = \"\"\n",
        "\n",
        "    data_size = data.shape[0]\n",
        "    # print(data_size)\n",
        "    for this_feature in data.columns[:-1]:\n",
        "        # shihtl> sort by each feature\n",
        "        this_data = data.sort_values(this_feature)\n",
        "\n",
        "        # shihtl> traversal all entries\n",
        "        for percent in range(percentile_count):\n",
        "            split_idx = min(data_size - 2, math.floor(data_size * percent * percentile_each))\n",
        "            # print(split_idx)\n",
        "\n",
        "            if (\n",
        "                this_data.iloc[split_idx][this_feature]\n",
        "                == this_data.iloc[split_idx + 1][this_feature]\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            this_threshold = (\n",
        "                this_data.iloc[split_idx][this_feature]\n",
        "                + this_data.iloc[split_idx + 1][this_feature]\n",
        "            ) / 2\n",
        "            this_mask = data[this_feature] <= this_threshold\n",
        "            this_ig = information_gain(data, this_mask)\n",
        "\n",
        "            # print(f\"{this_ig}, {this_threshold}, {this_feature}\")\n",
        "            # shihtl> find max ig\n",
        "            if this_ig > best_ig:\n",
        "                best_ig, best_threshold, best_feature = (\n",
        "                    this_ig,\n",
        "                    this_threshold,\n",
        "                    this_feature,\n",
        "                )\n",
        "        \n",
        "        # print(f\"Done {this_feature}!\")\n",
        "\n",
        "    best_ig = round(best_ig, 4)\n",
        "\n",
        "    # best_ig, best_threshold = float(best_ig), float(best_threshold)\n",
        "\n",
        "    return best_ig, best_threshold, best_feature\n",
        "\n",
        "\n",
        "# [Note] You have to save the value of \"ans_ig\", \"ans_value\", and \"ans_name\" into the output file\n",
        "# Here, let's try to find the best split for the input_data\n",
        "# Please round your answer to 4 decimal place\n",
        "# ans_ig, ans_value, ans_name = find_best_split_advance(input_data)\n",
        "# print(\"ans_ig = \", ans_ig)\n",
        "# print(\"ans_value = \", ans_value)\n",
        "# print(\"ans_name = \", ans_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# shihtl> COMMENT before hand in <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "from multiprocessing import Manager, Pool\n",
        "from itertools import product\n",
        "\n",
        "lock = Manager().Lock()\n",
        "pool = Pool(4)\n",
        "\n",
        "best_ig = -1e9\n",
        "best_threshold = 0\n",
        "best_feature = \"\"\n",
        "\n",
        "def process(data, feature, percent, lock):\n",
        "    global best_ig, best_threshold, best_feature\n",
        "    logger.info(f\"Start process {feature}, {percent}\")\n",
        "\n",
        "    data_size = data.shape[0]\n",
        "    this_data = data.sort_values(feature)\n",
        "\n",
        "    split_idx = min(\n",
        "        data_size - 2, math.floor(data_size * percent * percentile_each)\n",
        "    )\n",
        "    # print(split_idx)\n",
        "\n",
        "    if this_data.iloc[split_idx][feature] == this_data.iloc[split_idx + 1][feature]:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    this_threshold = (\n",
        "        this_data.iloc[split_idx][feature] + this_data.iloc[split_idx + 1][feature]\n",
        "    ) / 2\n",
        "    this_mask = data[feature] <= this_threshold\n",
        "    this_ig = information_gain(data, this_mask)\n",
        "\n",
        "    # shihtl> find max ig\n",
        "    # shihtl> sync need a lock\n",
        "    with lock:\n",
        "        if this_ig > best_ig:\n",
        "            best_ig, best_threshold, best_feature = (\n",
        "                this_ig,\n",
        "                this_threshold,\n",
        "                feature,\n",
        "            )\n",
        "        \n",
        "        logger.info(f\"Done process {feature}, {percent}\")\n",
        "\n",
        "def MP_find_best_split_advance(data):\n",
        "    \"\"\"\n",
        "    This function will find the best split combination of data\n",
        "    args:\n",
        "    * data(type: DataFrame): the input data\n",
        "    * impl_part(type: string): 'basic' or 'advanced' to specify which implementation to use\n",
        "    return\n",
        "    * best_ig(type: float): the best information gain you obtain\n",
        "    * best_threshold(type: float): the value that splits data into 2 branches\n",
        "    * best_feature(type: string): the feature that splits data into 2 branches\n",
        "    \"\"\"\n",
        "\n",
        "    global pool, lock\n",
        "    global best_ig, best_threshold, best_feature\n",
        "\n",
        "    best_ig = -1e9\n",
        "    best_threshold = 0\n",
        "    best_feature = \"\"\n",
        "\n",
        "    input_product = product([data], data.columns[:-1], range(percentile_count), [lock])\n",
        "    pool.map(process, input_product)\n",
        "    pool.join()\n",
        "\n",
        "    best_ig = round(best_ig, 4)\n",
        "\n",
        "    return best_ig, best_threshold, best_feature\n",
        "\n",
        "\n",
        "# shihtl> COMMENT before hand in >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tree_advance(data, max_depth, min_samples_split, depth, mp):\n",
        "    \"\"\"\n",
        "    This function will build the decision tree\n",
        "    args:\n",
        "    * data(type: DataFrame): the data you want to apply to the decision tree\n",
        "    * max_depth: the maximum depth of a decision tree\n",
        "    * min_samples_split: the minimum number of instances required to do partition\n",
        "    * depth: the height of the current decision tree\n",
        "    return:\n",
        "    * subtree: the decision tree structure including root, branch, and leaf (with the attributes and thresholds)\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    # check the condition of current depth and the remaining number of samples\n",
        "    if depth < max_depth and data.shape[0] > min_samples_split:\n",
        "        # call find_best_split_advance() to find the best combination\n",
        "        ig, threshold, feature = MP_find_best_split_advance(data) if mp else find_best_split_advance(data)\n",
        "\n",
        "        # check the value of information gain is greater than 0 or not\n",
        "        if ig > 0:\n",
        "            # update the depth\n",
        "            depth += 1\n",
        "\n",
        "            # call make_partition() to split the data into two parts\n",
        "            left, right = make_partition(data, feature, threshold)\n",
        "            # print(left)\n",
        "            # print(right)\n",
        "\n",
        "            # If there is no data split to the left tree OR no data split to the right tree\n",
        "            if left.empty or right.empty:\n",
        "                # return the label of the majority\n",
        "                label = int(data[\"hospital_death\"].mode().iloc[0])\n",
        "                return label\n",
        "            else:\n",
        "                question = \"{} {} {}\".format(feature, \"<=\", threshold)\n",
        "                subtree = {question: []}\n",
        "\n",
        "                # call function build_tree() to recursively build the left subtree and right subtree\n",
        "                left_subtree = build_tree_advance(left, max_depth, min_samples_split, depth, mp=mp)\n",
        "                right_subtree = build_tree_advance(right, max_depth, min_samples_split, depth, mp=mp)\n",
        "\n",
        "                if left_subtree == right_subtree:\n",
        "                    subtree = left_subtree\n",
        "                else:\n",
        "                    ans_features.append(feature)\n",
        "                    ans_thresholds.append(int(threshold))\n",
        "                    \n",
        "                    subtree[question].append(left_subtree)\n",
        "                    subtree[question].append(right_subtree)\n",
        "        else:\n",
        "            # return the label of the majority\n",
        "            label = int(data[\"hospital_death\"].mode().iloc[0])\n",
        "            return label\n",
        "    else:\n",
        "        # return the label of the majority\n",
        "        label = int(data[\"hospital_death\"].mode().iloc[0])\n",
        "        return label\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return subtree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "i1I7wbpWa_N1"
      },
      "outputs": [],
      "source": [
        "def build_forest(data, n_trees, n_features, n_samples, mp):\n",
        "    \"\"\"\n",
        "    This function will build a random forest.\n",
        "    args:\n",
        "    * data: all data that can be used to train a random forest\n",
        "    * n_trees: total number of tree\n",
        "    * n_features: number of features\n",
        "    * n_samples: number of instances\n",
        "    return:\n",
        "    * forest: a random forest with 'n_trees' of decision tree\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    global rng\n",
        "    data_len = data.shape[0] - 1  # shihtl> label\n",
        "    feature_list = data.columns[:-1]  # shihtl> label\n",
        "    forest = []\n",
        "\n",
        "    # shihtl> COMMENT before hand in <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "    logger.info(data.columns)\n",
        "    # shihtl> COMMENT before hand in >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Create 'n_trees' number of trees and store each into the 'forest' list\n",
        "    for i in range(n_trees):\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Select 'n_samples' number of samples and 'n_features' number of features\n",
        "        # (you can select randomly or use any other techniques)\n",
        "\n",
        "        selected_datas = rng.choice(data_len, size=n_samples, replace=False)\n",
        "        selected_features = rng.choice(feature_list, size=n_features, replace=False)\n",
        "        selected_features = np.append(selected_features, \"hospital_death\")\n",
        "\n",
        "        # global_seed += 30  # shihtl> to avoid all tree are identical\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        print(f\"selected_datas = {selected_datas}\")\n",
        "        print(f\"selected_features = {selected_features}\")\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Store the rows in 'selected_datas' from 'data' into a new DataFrame\n",
        "        tree_data = pd.DataFrame(data, index=selected_datas, columns=selected_features)\n",
        "        # for idx in selected_datas:\n",
        "        #     tree_data = pd.concat([tree_data, data.iloc[idx]], ignore_index=True)\n",
        "\n",
        "        # Filter the DataFrame for specific 'selected_features' (columns)\n",
        "        tree_data = tree_data[selected_features]\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Then use the new data and 'build_tree' function to build a tree\n",
        "        tree = build_tree_advance(tree_data, max_depth, min_samples_split, depth, mp=mp)\n",
        "        print(tree)\n",
        "\n",
        "        # Save your tree\n",
        "        forest.append(tree)\n",
        "\n",
        "    return forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "WvV8U7C2bIqO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-09 01:36:48 | __main__ | INFO | NOT MP start!\n",
            "2024-10-09 01:36:48 | __main__ | INFO | Index(['age', 'bmi', 'gender', 'height', 'weight', 'pre_icu_los_days',\n",
            "       'arf_apache', 'bun_apache', 'creatinine_apache', 'gcs_eyes_apache',\n",
            "       'gcs_motor_apache', 'gcs_unable_apache', 'gcs_verbal_apache',\n",
            "       'glucose_apache', 'heart_rate_apache', 'hematocrit_apache',\n",
            "       'intubated_apache', 'map_apache', 'resprate_apache', 'sodium_apache',\n",
            "       'temp_apache', 'ventilated_apache', 'wbc_apache',\n",
            "       'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob', 'aids',\n",
            "       'cirrhosis', 'diabetes_mellitus', 'leukemia', 'hospital_death'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "selected_datas = [ 872 7088 2952 3681 7697 1740  121 1486 2987 4123 4118 6965 7571 4558\n",
            " 4481 2240 4486 2418 6291 7034 6482 7743 7854 6874 8069 6887 1175 5617\n",
            " 4466 4052 6439 3965 7870 4991 3106 3887 4847 2364 2821 3884 5802 7923\n",
            " 1201 7261 7758  932 4642 1882 2623 4370 4798 4771 7714 3400 2650 3537\n",
            "  138 4452 6191 4373 4752 2531 3688 3712 1821 3388 4827 5400 2322 2792\n",
            " 1434 1448 3932 5191 4814 3413 4658 1140 2941 3676 7500 4053 1230 4828\n",
            " 2371  760 2505 3762 4647 5349 5677 4810 5317  776 4926 4678  787 4809\n",
            " 2082 1749 6194 3409 5150 6098 2120 4185 5687 2381  375 2175  336 7298\n",
            " 5495 5623 2419 2125 2036 7684 2294 7505 4161 3605 1518 7499 6649 2428\n",
            " 3776 3675 3978 3292 2958 1311  888 4821 7285 1045  731  755 3100 4501\n",
            " 6526 6440 1728 2307 2404 2407  234 6334 7388 5987  341 6113 1085 4856\n",
            " 3782 4654 7272 7561  112 5901 4686 5225  865 2713 7795 2138  259 6232\n",
            " 7798 4436 2016  836 3192 6292 4749 7405 7662 6425 4498 4549 7002 3573\n",
            " 3952 1865 7953 6765  272 1110 5861 7402  187 1854 2171 3093 1078 6723\n",
            " 1368 6298 5489 3580 2846 6613 2305   37 7510 5851  936 6325 2725 5224\n",
            " 2720 3314 3860  576 7377 1393 7325 6620 7718 4631 5110 6033 1291 6802\n",
            " 7112 7962 7314 1248 3120 6315 7081 1812 5314 5067  473 7664  859 6652\n",
            " 3011 4632 3189  765 7839 2947 3237 3071 6487 3417 4216  495 5327 4578\n",
            " 8041 7647 4569  780 4414 5100  769 8023 5114 7717 1831  520 1513 6720\n",
            " 6464  481 1961 7769 3883 6977 2281 5892 2957  456 3449 1111 3809 6116\n",
            " 3706 5905 3214 2403  586 2074 5868 1463 3946 4122 6103 7917 3904    4\n",
            "  758 7678 7655 3242  606 2644 4720 2838 3159 1118 4153  409 3998 1402\n",
            " 3004 2487  215 1853 4243 5462 1186 4531 7099 4831 5691 1002 2574 5373\n",
            " 6691 6645 2884 2585 7877 4762  422 1015  197 2319 1500 1253  923 2558\n",
            " 2579 6070 6037  696 6117 7986 4872 1436  693 1682  672 7846 3715 3802\n",
            " 1852 7407 4751  322 1218 2600  745  417 3705 6834 2069 7669 4296 6010\n",
            " 3018  868 4511 5698 4315 3803 1124 7297 1162 4899  446 5429 1676 2545\n",
            " 7618 6591 3371 5911 1086  552 7619 1195 1613 6763 2643  440 7952 8017\n",
            " 2295 6689 4755 1179 3080 3823 5295  833 2081 3524 4775 5928 1361  133\n",
            " 3913 2040 7048 7929 1838 1084   61 5398 2097 2968 5585 7288 5998 6654\n",
            " 6378 3131 5825 3290 2722  392 7005 6309 5597 2781 5006 7040 5942 2549\n",
            " 7319 5791 1594  380 7749 4374 4212 1760 5961 7237 8020 7591 6602 5752\n",
            " 7408  747 3139 2076 3454 7675  862 6005 7508 2543 3202 5941 6271 7565\n",
            " 2728 1546 1952 1595  363 2300 2951 6465  510 4002 1575 3088 1620 1548\n",
            " 4758 7838 3112  730 6721  242 2782 1488 1295 3378 5294 3457 5656 6335\n",
            " 2155 1205 6218 5313 5384 8018 5322  715 1178  847 5095 2964 8035 3359\n",
            " 2004  419 4535  221 7196 5128 3696 6131 7821 7440 3432  753 6432 1945\n",
            " 2787  650  189 1870 5254 7094 2652 6589 4493 2609 1856 4520  601 3262\n",
            " 7139 1289 1168 3052 7801 6106 1699 1412 4325 3857 5328 6685 4395 5514\n",
            " 4950 7203 2378 4095 4295 7753 2739  533 7076 4660 2616 1214   33 2153\n",
            " 1499 2188    6 3354  794 5668 2619 3282 4633 1246 4570  712 4548 6319\n",
            " 6759 2461 6462 3398 3005 2602 4432 5021  153 3302 3761 1734    7 7136\n",
            " 1032 1365  115 1386 5515 2289  627 2954 3221 4974  493 5748  654 5077\n",
            " 7299 5037 1504 5510 1147 3085 6523 6149 7719 7603 6327 6312 1859 1713\n",
            " 2485   41 7587  233 5544 4891 1625 3236 6498 6919 7142 5201 5511 4412\n",
            "   35 5113  900 2394 5156 6856 6460 6420 4461 5693 1512  595 6807 5068\n",
            " 5731   75 1401 7117 5232 1881 5350  781 5570 4863 5600 1823 2104 1690\n",
            " 2985 1125 1932 3664 4518 2672 3265  914 8051 5614 5347 3900  315 7479\n",
            "  695 2409 6843 1475 2973 7944 6209 3737 1001 2207 3220 7789 5819 1451\n",
            " 1635 3807 1352 7269   59 4562  152 6127 1864 7341 3881 1350 2960 4232\n",
            " 1873 7226 5984 2852 3433 3323 4927 5106 4585 7158 4556 5143 3308 7134\n",
            " 8002 6332  727 7021 3686  946 4730   88 5249 3784 5577 6318 2962 7045\n",
            " 4709 2917 2621 4088 1605 5586 4763  832 1422 2576 4538 6923 7146 5243\n",
            "  698  874 6491 5847 3755 2258 3201 1712 6777 6167 4089 2027 7182 3661\n",
            " 6156 6057 2501 2464 2374  717  861 7976 1371 4618 1759 1524 6896 2661\n",
            " 4283 1376 3113 3621 7578 2469 4940  281 5814 2414 2031  469 6894 7558\n",
            " 1271 5019 2745 6227 2202 6966 8034 4138  364 2970 2649 2342 6320 3994\n",
            " 6729 6181 1710 1974  298 3176 4773  299 5920]\n",
            "selected_features = ['gcs_motor_apache' 'age' 'gender' 'pre_icu_los_days' 'hospital_death']\n",
            "{'pre_icu_los_days <= 1.2024305554999999': [{'pre_icu_los_days <= 0.2833333335': [{'age <= 83.5': [0, 1]}, 0]}, 1]}\n",
            "selected_datas = [7684 7185 7567  403 6486 4526 2706 7707 6058 7661 7489 7457 4011 6129\n",
            " 2406 1494 1832 6409 2399  258 2587 3796 1457 2553 6432 1625 4192 2051\n",
            " 3671 3014 6445 3334 1820 6370 7694 7845 8002 5333 1590 7710 5295 7098\n",
            " 7334 3871 7828 4550 2199 8066 2225 1308 4256 2394 5592 3059 7251 2015\n",
            " 6928 2763 2894 7373 3911 4601 6119 5384 5412 6811 6571 5452 4714 4860\n",
            " 4308 5133 7723 2693 4858 3394 7483 3026  768 6366 7084 7968 1780 2726\n",
            " 4607 2931 1506 2329 7751 3804 3163 6544 4358 4241  567 2422 5873 7601\n",
            " 3904 2301 5172 5862 3338 2100 4481 4562 1495  500 2972 4349 5289 3564\n",
            " 1864 4499  376 2974  826 5770 2357 3456 4986 1619 2078 1997 6475 6597\n",
            " 5397 3649 6685 7670 2323 3009 2667 2265 5728 2163 5924  941 3912 6231\n",
            " 7228 3135 3685   30 1488 7993 5874 1472 1111 4177 4887 7310 1718  940\n",
            " 4410 4753  547 7307 2538 5460 5097 1554 5894 4114 5351 2563 2172 2516\n",
            " 7459 2416  659 5655 3870 7194 6096 4867 1991 3006 4455 1929  651 1034\n",
            " 1774 2235 6458 1569  494 7405 4941  860 4583 5600  216 1050 6648  889\n",
            " 3950 1431 1066  847 7702 2330  426 4968 3825 1019 3627 4991 2781 6756\n",
            " 2922 6278 3898  361 2827 5791  914  975 5378 1826 5285 4416 6188 1338\n",
            " 7026 7808 1563 4581 5003 5473 5274  331 7283 2683 2452 4482 3086 4613\n",
            " 1317 3483 2513 1145 5404  243 2629 1227 7775 3546 6255 2770 5890 4902\n",
            " 7454 4574 2680 7472 4272 7462 7639 5534 3303 7370 7366 6889 1675 1954\n",
            " 1616 4062 2537 2428 4643 4492 6920 2254  296 3173 5414 1762 2264 1350\n",
            " 1841  265 2691 1011 3275 5244 7060 8054  370 6657 3883 6979 1362 2625\n",
            " 7318 4222  393 4501  570 4375 5314 2723 4331 3455 6386 6780 7748 5415\n",
            "  780 2108 1072 3812 5207  312 4680 5459 1531 2918 7163 4160  782 1514\n",
            " 4945 3495 6121 2026 7007 3178 7944  937 3834  490 3742 2031 2973  581\n",
            " 4622 1755 6234 1802 7689  901 3491 5658 1628  300 4451 1572 3923 2802\n",
            " 3724 7621 7474 3345 6694 4700  391 1989 6369 3434 4974 3715 2639 6483\n",
            " 1781 2074 2152 8045 4720 1162 6829  149 1529 3551 6663 2156 3743 3466\n",
            " 4458 1122 5259  352 1223 7776  146 2342 3330 6076  729 5677 2286   43\n",
            " 5554  900 7171 7786 3682 4030 4617 1697 2494 1133 1064 3896 7837 5396\n",
            " 5021 4298 1669 7536 6194 7092 3158 4535 1745 3209 1897 1233 3949 5824\n",
            " 5591 5278 2065 3659 6852 4361 5551 3401 4999 7061 6966  194  228 2868\n",
            " 5247  906 2715 5293 3689 3192 7272 5499 4498 7753 4427 6148 2511 3772\n",
            " 3040 1835 3908  992 5262 4771 5235 1877 7663 3770 4503 3816 4181 1651\n",
            " 5034 2731 2444  551 7024 4706 1888 4204  242 4685 2125 4896 3846 3211\n",
            " 8005 6074 6711 3688 4291 2898 4610 5102 5997 5192 6824 5975 6411 5131\n",
            " 3503  626 7539 3845 6633 3378 5544 6647 3344 8009 2072 3890 2805  439\n",
            " 5709 4981 4296 7836 2696 7606  884 2320 3056 3862 6865   58 2198  877\n",
            " 4553 6008 1971 1976 2023 5399  436 2157  630 7132 1260 6720 3747 3430\n",
            " 2179 2290 7535 2283 1025 3662 6846 6703 1040 2782 7158 5567 7202 2404\n",
            " 2253 2725 1993  850 7019 4709 4118 4207 2518 4772 6319 4472 2671 2953\n",
            "  887 5044 7088 7040 3520 7303  306 2417 6390 6536 5964 4584 4494 5641\n",
            "  519 7087 7573 2878 2799  864 2075 1643 6334  603 5474 4600 6568 2662\n",
            " 4846 7690 6114  709  730 7986 6854 2144 4242 5902 3057 7592  538 4735\n",
            " 7047 7271 3673 1458 7613 1037 7390 5194 1886  184 3255  367 5923 5999\n",
            " 5302 4783 3897 7286 1061 6918 6602 5572  181  404 6514 2340 4927 3248\n",
            " 8021 2583 4255 2280 1112 1596 3710 6186  969 3299 1285 6442 7424 3598\n",
            " 3002 4585 1772 7478 5064 3702 5606 7527 3340  924 3318 1459 6624 7434\n",
            " 2036  104 1013 7797 3412 3760 4598 2269 4663 1441 1456 3586 2871 2312\n",
            " 4890 7025   57 1940 5001 3364 6578 2692 3129 2336 6189  406 4751 7643\n",
            " 1779 7138 5305 1938 4133 2106  611 7062 1203 4901  734 6098 1806 3619\n",
            " 1095 2991 6471 7050 3452 4588  797 7294 1830 4667 5226 2027 4295 7909\n",
            "  544 8017 6867  432 2529 6147 1910 3310 1097  384 5408 7053 1208 5079\n",
            " 2870 5651 7795   33 7243 5900 4976 6089 6434 1974 4443 4419 3741 1353\n",
            " 7813 6704 2752  664 3621 7577 1693 6269 1551 7846 4952 5163 7198 7341\n",
            " 4528 2070 2321  803 5940 6422 7533 6754 5453 6550 7078 5547 2841 2801\n",
            " 8059  515   14 5019 6192 4351 1261 3076 5283 4596 5214  533  441 5425\n",
            " 4132 2099 4071 4383 7794 3373  411 4135 2944 2436 2949  972 4046 2954\n",
            " 6230 2888 7130  880  607 5877 7258 2761 2950]\n",
            "selected_features = ['glucose_apache' 'pre_icu_los_days' 'gcs_eyes_apache' 'intubated_apache'\n",
            " 'hospital_death']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-09 01:36:49 | __main__ | INFO | NOT MP done! cost 0.9670642999699339\n",
            "2024-10-09 01:36:49 | __main__ | INFO | MP start!\n",
            "2024-10-09 01:36:49 | __main__ | INFO | Index(['age', 'bmi', 'gender', 'height', 'weight', 'pre_icu_los_days',\n",
            "       'arf_apache', 'bun_apache', 'creatinine_apache', 'gcs_eyes_apache',\n",
            "       'gcs_motor_apache', 'gcs_unable_apache', 'gcs_verbal_apache',\n",
            "       'glucose_apache', 'heart_rate_apache', 'hematocrit_apache',\n",
            "       'intubated_apache', 'map_apache', 'resprate_apache', 'sodium_apache',\n",
            "       'temp_apache', 'ventilated_apache', 'wbc_apache',\n",
            "       'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob', 'aids',\n",
            "       'cirrhosis', 'diabetes_mellitus', 'leukemia', 'hospital_death'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'glucose_apache <= 171.5': [{'glucose_apache <= 69.5': [1, {'pre_icu_los_days <= 1.0256944445': [{'pre_icu_los_days <= 0.0225694445': [0, {'gcs_eyes_apache <= 1.5': [1, 0]}]}, 0]}]}, {'pre_icu_los_days <= 0.5340277775': [{'gcs_eyes_apache <= 3.5': [1, 0]}, 1]}]}\n",
            "selected_datas = [6059 4889 6390 2971 2389 8052 2754 3231 6986 2849 3688 1565 5787 6448\n",
            " 4993 6748 1982 7879 3434 3373  948 4788  604 1082 6495 7166 3729 7642\n",
            " 3058 4634 3055 6512 2763 4140 1914 4018 3545 4981 5341 5294 5339  515\n",
            " 1869 1683 6947 6851 4524 1599  907 4697 4912  902 3741  993 7559 5595\n",
            "  176 7632 4283 1181 1510 5103  463  298 1389  662  657 7092   46 3999\n",
            " 5699 6383   15 8059 1047 3806 2194 2488 4689 3746 1591 1002 1386 1376\n",
            " 2464 6173 5540 4557 2383 4696 4266 2023  978 7940 5057 7701  773  318\n",
            " 2936 7972 1062 5794 5130 5564 3396 1022 2055 4086  507 6719 5686 2259\n",
            " 5904 1952 4282 3776  189 1790 6795  194 5843  414 3002  569 7526 6051\n",
            " 7986 6584 6421 7090 5071 6350 1721 7032 7080 2789 7204 2159  503 2756\n",
            " 7569  872  713  620 3989 7377 6143 5332 6381  109 2256 5948 7459 1965\n",
            " 7085 4822  806 7054  429  893 3781 7589 2956 6114 1737 1549  392  473\n",
            " 4052 1935 6166 5288 2122 2827 5934 4566 3824 7127 3138 6781 6746 7876\n",
            "  656 4948 2506 5237  592 7637 2991 7924 1874 1829 7328 7672 7223 2671\n",
            " 2736 7851 7077  655 7452 3619 6899  789 3649 7962 3855 7094 7682 3964\n",
            " 5224 5312  917 1033 7155  372  268 7671 8056  399 2559 3401  329 7884\n",
            " 6461 6095 2660 3810 6764 3540 7038 6853 2946 3377  401 5320 6141 3584\n",
            " 2514 3766 1761  866 2707 2005 5384 2016 3057 7374  369 4197 6217 1590\n",
            " 7739 2335 4270  757 1897 1867  736 7673 3495 1387 7542 1315 5552 6500\n",
            "  800 7455 2178 6439 4493   37 2498 5366 7353 1036 2893 3992 6054  195\n",
            " 5718 5365 3252 3509 7243 2479 6998 3909 4898 7845 6002 5736 7704 5885\n",
            " 1265 5434 4702 1483 3416 2314 4598 4237 7110 4101 6196 2097  563 2200\n",
            " 7772 6313 2891 6045 7366 1284 2695  184 3467 3172 8061 5043 6412 1211\n",
            " 4472  989 4174 4854 3842 4823 3926 4080 1158 6192  451 2778 5042 6413\n",
            " 5301 3109 2503 5986 6838 4454 6405 4599 5678 5989 7304 7220 6537 7315\n",
            " 2476 3546 7862 4582 2190 3621 6966 6891 1891 5533 3263 4663 6456 5149\n",
            " 5449 3009  124 8073 4924   24 5289 2650 4877 2489 1141 7685  883   38\n",
            " 5436 6205 2927 5606  201 4722 6506 3993 2203 7773 2440 7501 2292 1629\n",
            " 4580 6268  188 6745 3774 7052 1500 3198 3627 2629 5516 2844 4602 6470\n",
            " 2246 6102 3094 1560  542 2378 3022 5199 5663 6139 4357 2566  187 7870\n",
            " 6171 4876 6011 2211  897 3046 2069 5252 1418 3388 5104 6551 1296 3771\n",
            " 5719 2923 3160 3569   60 5024 4986 2341 1428 5874 6158 7411 5706 5343\n",
            " 7165 7566   39 2600 5735  875 2586 5156 3313 2044 2326 7064 4041 1623\n",
            " 5038 3248 6096 4501 4257 2036 4181 2123 7371 7056 5955 6813 3834 4633\n",
            " 1157 1159  804 4388  116 7763  441 5396 4520 1576 4742 5166 2995 4933\n",
            " 3008 2710 3946 6290 7235 1775 1403 4801 7791 5601 2896  671 7860 6042\n",
            " 4167 7392 7638 5819  191 7541 2281 7690 5059 3651 6497 3092 6588 6075\n",
            " 2467  357 2656 5764 5782 7126  175 5109 6220 1686 7725 5589 6779  782\n",
            " 1209 2516 3921 2402 2384 4128 2280 7921 3949 4552 7848 6039 4535 3948\n",
            " 2933 2088 1049  529 1447  723 1569 7161 3899 5875 7913 2952 1593 1231\n",
            " 6326  822 1800 1916 2555 4755 2019 7660  396 3823 6293 7609 6529 6965\n",
            " 1163 5257 1805 5354  615 2355 1722 5857 3965 3782 4846  164 4478 4363\n",
            " 1208  351 3559 1191 5202  519 3428 4828 1184 3026 1917 5398 5314 6442\n",
            " 2490 4364 5227 6599 1503 3615 3191 1763 5769 7775  791 6343 4410 5407\n",
            " 7264 3079 5864 4182 3532 5003 7096 2064 6970 3017 4951 2937 2934 7247\n",
            " 4458 4461 7784 4076 1256 7057 5496 3967 3163 6127 4494 7588 2912 7856\n",
            " 3549 1375 3074   83  960 5887 5893 7692 7051 3867 1908 6542  771 6682\n",
            " 2637 7268 1342 7980 4240 5238 2222 2884 2099 7752  688 4642 1745 4903\n",
            " 3151 6652 6133 1330 4825 1016 7291 5345 2811 5233  156 6854  242  512\n",
            " 6603 2092 1716  698 5994 7135 4938 3580 1837 2964 1096 3591 7042 1741\n",
            " 1886 7874 6849 2347 8042 5344 6360 7024 3577 1192 1674  445 5006 4954\n",
            " 1993 2681 4153 3845 6692 6146 2132 5506 7795 3815 1622 6660 7744 2048\n",
            " 7252 5493 4613 6691 5415 4604  981 2641 2456 2324 5229 4045 4721 3954\n",
            " 7380 2727 4576 4859 6447 6367 2847  788 6994 1370 2881 4098 1555 1688\n",
            " 3581 6013 4308 5741 1595   18  969 1682 5698 5711 6809  338 1757 7978\n",
            " 3447 4716 5086 6869 6454 4559 7794  856 7768 3758 5982 5768 2126 2890\n",
            " 2063 3783 5017 2821 3116 1206  772 6199 5737  301 6676 6017 2596 2785\n",
            "  727 7170 4870 5858 5077 3706 2477 4738 6336]\n",
            "selected_features = ['sodium_apache' 'bmi' 'gcs_verbal_apache' 'intubated_apache'\n",
            " 'hospital_death']\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "logger.info(\"NOT MP start!\")\n",
        "start = time.perf_counter()\n",
        "forest = build_forest(training_data, n_trees, n_features, n_samples, mp=False)\n",
        "end = time.perf_counter() - start\n",
        "logger.info(f\"NOT MP done! cost {end}\")\n",
        "\n",
        "logger.info(\"MP start!\")\n",
        "start = time.perf_counter()\n",
        "forest = build_forest(training_data, n_trees, n_features, n_samples, mp=True)\n",
        "end = time.perf_counter() - start\n",
        "logger.info(f\"MP done! cost {end}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD0v4Af4bM_T"
      },
      "source": [
        "### Step 4: Make predictions with the random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "45zA7JFVbRLr"
      },
      "outputs": [],
      "source": [
        "def make_prediction_forest(forest, data):\n",
        "    \"\"\"\n",
        "    This function will use the pre-trained random forest to make the predictions\n",
        "    args:\n",
        "    * forest: the random forest\n",
        "    * data: the data used to predict\n",
        "    return:\n",
        "    * y_prediction: the predicted results\n",
        "    \"\"\"\n",
        "    y_prediction = []\n",
        "    predictions = []\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Loop through each tree in the forest\n",
        "    for tree in forest:\n",
        "        # Call 'make_prediction'\n",
        "        pred = make_prediction(tree, data)\n",
        "        predictions.append(pred)\n",
        "\n",
        "        # shihtl> COMMENT before hand in <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "        logger.info(pred)\n",
        "        # shihtl> COMMENT before hand in >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "    # Here, each tree has made its predictions.\n",
        "    # We can use majority vote in which the final prediction is determined by the mode (most frequent prediction) across all the trees.\n",
        "    # Feel free to use any other method to determine the final prediction\n",
        "\n",
        "    predictions = pd.DataFrame(predictions)\n",
        "    y_prediction = predictions.mode().values[0].tolist()\n",
        "    # shihtl> COMMENT before hand in <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "    logger.info(f\"{y_prediction}\")\n",
        "    # shihtl> COMMENT before hand in >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "    # # Loop through each column of 'predictions'\n",
        "    # for pred in predictions:\n",
        "    #     # For a specific column, find out each tree's prediction\n",
        "    #     column_predictions = np.average(pred)\n",
        "    #     logger.info(column_predictions)\n",
        "    #     # Then, use a method to determine the final prediction for this column\n",
        "    #     # append the final prediction to y_prediction\n",
        "    #     if column_predictions > 0.5:\n",
        "    #         y_prediction.append(1)\n",
        "    #     else:\n",
        "    #         y_prediction.append(0)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return y_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXED6E837NRk"
      },
      "source": [
        "Validation (Optional)\n",
        "> If you split the data into training and validation sets in step 2, you can assess the accuracy of the forest here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CsC39J9P7h-j"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-09 01:35:21 | __main__ | INFO | [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
            "2024-10-09 01:35:21 | __main__ | INFO | [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1]\n",
            "2024-10-09 01:35:21 | __main__ | INFO | [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "2024-10-09 01:35:21 | __main__ | INFO | [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "2024-10-09 01:35:21 | __main__ | INFO | 0.2026431718061674\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "validation_data_x = validation_data.drop(\"hospital_death\", axis=1)\n",
        "validation_data_y = validation_data[['hospital_death']]\n",
        "\n",
        "pred_validation = make_prediction_forest(forest, validation_data_x)\n",
        "# shihtl> COMMENT before hand in <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "logger.info(pred_validation)\n",
        "# shihtl> COMMENT before hand in >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "score = calculate_score(validation_data_y, pred_validation)\n",
        "# shihtl> COMMENT before hand in <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "logger.info(score)\n",
        "# shihtl> COMMENT before hand in >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqp8xzDJTV6S"
      },
      "source": [
        "After you have completed fine-tuning and validating the forest, you can proceed to make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "atRpa9KoPgNP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-09 01:35:21 | __main__ | INFO | [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
            "2024-10-09 01:35:21 | __main__ | INFO | [1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "2024-10-09 01:35:21 | __main__ | INFO | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "y_pred_test = make_prediction_forest(forest, advanced_testing_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfLRewfzbjiZ"
      },
      "source": [
        "### Step 5: Write the Output File\n",
        "Save your predictions from the **random forest** in a csv file, named as **lab2_advanced.csv**\n",
        "> Note: Please do not touch the code in this step, we have made sure this outputs the correct file format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_H6MNjNmbst1"
      },
      "outputs": [],
      "source": [
        "advanced = []\n",
        "for i in range(len(y_pred_test)):\n",
        "  advanced.append(y_pred_test[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "7DHteTW7bvxz"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hospital_death</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     hospital_death\n",
              "Id                 \n",
              "0               0.0\n",
              "1               0.0\n",
              "2               0.0\n",
              "3               0.0\n",
              "4               0.0\n",
              "..              ...\n",
              "895             0.0\n",
              "896             0.0\n",
              "897             0.0\n",
              "898             0.0\n",
              "899             0.0\n",
              "\n",
              "[900 rows x 1 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advanced_path = 'lab2_advanced.csv'\n",
        "\n",
        "advanced_df = pd.DataFrame({'Id': range(len(advanced)), 'hospital_death': advanced})\n",
        "advanced_df.set_index('Id', inplace=True)\n",
        "advanced_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "AZWdWt5fGPe9"
      },
      "outputs": [],
      "source": [
        "advanced_df.to_csv(advanced_path, header = True, index = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
